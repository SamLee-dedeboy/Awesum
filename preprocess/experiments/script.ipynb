{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import distances, eval, llm, overlaps, word_movers_distance\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "dataset = json.load(open('data/pairwise_evaluation_w_embeddings.json'))\n",
    "# dataset format:\n",
    "# [\n",
    "#  { \n",
    "# article_id: str, writer_id: str, evaluator_id: str,\n",
    "# article_text: str, writer_summary: str, text-davinci-002_summary: str,\n",
    "# overall_writer_better: bool | \"Equally Good\", informative_writer_better: bool | \"Equally Good\",\n",
    "# full_embedding: [float], writer_summary_embedding: [float], llm_summary_embedding: [float] \n",
    "# ]\n",
    "\n",
    "# test scripts\n",
    "def test_cosine_distance(dataset, epsilon=0.001):\n",
    "    confusion_matrix_overall = eval.llmConfusionMatrix()\n",
    "    confusion_matrix_informative = eval.llmConfusionMatrix()\n",
    "    for datum in dataset:\n",
    "        full_embedding = np.array(datum['full_embedding'])\n",
    "        writer_summary_embedding = np.array(datum['writer_summary_embedding'])\n",
    "        llm_summary_embedding = np.array(datum['llm_summary_embedding'])\n",
    "        # cosine distance\n",
    "        d1 = distances.cosine_distance(full_embedding, writer_summary_embedding)\n",
    "        d2 = distances.cosine_distance(full_embedding, llm_summary_embedding)\n",
    "        pred = distances.cosine_distance_writer_better(d1, d2, epsilon)\n",
    "        confusion_matrix_overall.add(datum['overall_writer_better'], pred)\n",
    "        confusion_matrix_informative.add(datum['informative_writer_better'], pred)\n",
    "    return confusion_matrix_overall, confusion_matrix_informative\n",
    "\n",
    "def test_linear_regression(dataset, epsilon=0.001):\n",
    "    confusion_matrix_overall = eval.llmConfusionMatrix()\n",
    "    confusion_matrix_informative = eval.llmConfusionMatrix()\n",
    "    # linear regression\n",
    "    full_embeddings = np.array([datum['full_embedding'] for datum in dataset])\n",
    "    writer_summary_embeddings = np.array([datum['writer_summary_embedding'] for datum in dataset])\n",
    "    llm_summary_embeddings = np.array([datum['llm_summary_embedding'] for datum in dataset])\n",
    "    # fit linear regression between full and writer summary and calculate distance\n",
    "    writer_distances = distances.linear_regression_distances(full_embeddings, writer_summary_embeddings)\n",
    "\n",
    "    # fit linear regression between full and llm summary and calculate distance\n",
    "    llm_distances = distances.linear_regression_distances(full_embeddings, llm_summary_embeddings)\n",
    "    predictions = list(map(lambda writer_distance, llm_distance: distances.linear_regression_writer_better(writer_distance, llm_distance, epsilon), writer_distances, llm_distances))\n",
    "    overall_writer_better_labels = [datum['overall_writer_better'] for datum in dataset]\n",
    "    informative_writer_better_labels = [datum['informative_writer_better'] for datum in dataset]\n",
    "    confusion_matrix_overall.addList(overall_writer_better_labels, predictions)\n",
    "    confusion_matrix_informative.addList(informative_writer_better_labels, predictions)\n",
    "\n",
    "    return confusion_matrix_overall, confusion_matrix_informative\n",
    "\n",
    "# def test_mover_score(dataset, epsilon=0.001):\n",
    "#     confusion_matrix_overall = eval.llmConfusionMatrix()\n",
    "#     confusion_matrix_informative = eval.llmConfusionMatrix()\n",
    "#     for datum in dataset:\n",
    "#         full_text = datum['article_text']\n",
    "#         writer_summary = datum['writer_summary']\n",
    "#         llm_summary = datum['text-davinci-002_summary']\n",
    "#         # writer summary mover score\n",
    "#         writer_mover_score = moverscore.corpus_score(full_text, writer_summary)\n",
    "#         llm_mover_score = moverscore.corpus_score(full_text, llm_summary)\n",
    "#         pred = moverscore.predict_writer_better(writer_mover_score, llm_mover_score, epsilon)\n",
    "#         confusion_matrix_overall.add(datum['overall_writer_better'], pred)\n",
    "#         confusion_matrix_informative.add(datum['informative_writer_better'], pred)\n",
    "#     return confusion_matrix_overall, confusion_matrix_informative\n",
    "\n",
    "def test_rogue(dataset, epsilon=0.001):\n",
    "    confusion_matrix_overall = eval.llmConfusionMatrix()\n",
    "    confusion_matrix_informative = eval.llmConfusionMatrix()\n",
    "    for datum in dataset:\n",
    "        full_text = datum['article_text']\n",
    "        writer_summary = datum['writer_summary']\n",
    "        llm_summary = datum['text-davinci-002_summary']\n",
    "        # rogues\n",
    "        writer_rogue = overlaps.rogue_score(full_text, writer_summary)\n",
    "        llm_rogue = overlaps.rogue_score(full_text, llm_summary)\n",
    "        # print(writer_rogue, llm_rogue)\n",
    "        pred = overlaps.writer_better(writer_rogue, llm_rogue, higher_better=True, epsilon=epsilon)\n",
    "        confusion_matrix_overall.add(datum['overall_writer_better'], pred)\n",
    "        confusion_matrix_informative.add(datum['informative_writer_better'], pred)\n",
    "    return confusion_matrix_overall, confusion_matrix_informative\n",
    "\n",
    "def test_wmd(dataset, wmd_scorer, epsilon=0.001):\n",
    "    confusion_matrix_overall = eval.llmConfusionMatrix()\n",
    "    confusion_matrix_informative = eval.llmConfusionMatrix()\n",
    "    for datum in dataset:\n",
    "        full_text = datum['article_text']\n",
    "        writer_summary = datum['writer_summary']\n",
    "        llm_summary = datum['text-davinci-002_summary']\n",
    "        # rogues\n",
    "        writer_wmd = wmd_scorer.distance(full_text, writer_summary)\n",
    "        llm_wmd = wmd_scorer.distance(full_text, llm_summary)\n",
    "        pred = wmd_scorer.predict_writer_better(writer_wmd, llm_wmd, epsilon)\n",
    "        confusion_matrix_overall.add(datum['overall_writer_better'], pred)\n",
    "        confusion_matrix_informative.add(datum['informative_writer_better'], pred)\n",
    "    return confusion_matrix_overall, confusion_matrix_informative\n",
    "\n",
    "\n",
    "def test_llm(dataset, api_key):\n",
    "    confusion_matrix_overall = eval.llmConfusionMatrix()\n",
    "    confusion_matrix_informative = eval.llmConfusionMatrix()\n",
    "    llmEvaluator = llm.LLMEvaluator(api_key)\n",
    "    for index, datum in enumerate(dataset):\n",
    "        full_text = datum['article_text']\n",
    "        writer_summary = datum['writer_summary']\n",
    "        llm_summary = datum['text-davinci-002_summary']\n",
    "        # writer summary mover score\n",
    "        pred = llmEvaluator.predict_writer_better(full_text, writer_summary, llm_summary)\n",
    "        print(index, pred)\n",
    "        confusion_matrix_overall.add(datum['overall_writer_better'], pred)\n",
    "        confusion_matrix_informative.add(datum['informative_writer_better'], pred)\n",
    "    return confusion_matrix_overall, confusion_matrix_informative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with cosine distance\n",
    "epsilons = [0.01, 0.001, 0.0001, 0.00001, 0.000001, 0]\n",
    "for epsilon in epsilons:\n",
    "    confusion_matrix_overall, confusion_matrix_informative = test_cosine_distance(dataset, epsilon=epsilon)\n",
    "    print(epsilon, confusion_matrix_overall.get_accuracy(), confusion_matrix_informative.get_accuracy(), confusion_matrix_overall.get_accuracy_by_label(label=True), confusion_matrix_overall.get_accuracy_by_label(label=False), confusion_matrix_overall.get_accuracy_by_label(label=\"Equally Good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599,)\n",
      "(599,)\n",
      "10 0.19532554257095158 0.22036727879799667 0.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# test with linear regression\n",
    "epsilons = [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0]\n",
    "for epsilon in epsilons:\n",
    "    confusion_matrix_overall, confusion_matrix_informative = test_linear_regression(dataset, epsilon=epsilon)\n",
    "    # print(confusion_matrix_overall)\n",
    "    print(epsilon, confusion_matrix_overall.get_accuracy(), confusion_matrix_informative.get_accuracy(), confusion_matrix_overall.get_accuracy_by_label(label=True), confusion_matrix_overall.get_accuracy_by_label(label=False), confusion_matrix_overall.get_accuracy_by_label(label=\"Equally Good\"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with mover score\n",
    "# 0.01 0.2637729549248748 0.24874791318864775\n",
    "# 0.001 0.2888146911519199 0.2687813021702838\n",
    "# 0.0001 0.29549248747913187 0.27545909849749584\n",
    "# 1e-05 0.2988313856427379 0.27879799666110183\n",
    "# 1e-06 0.2988313856427379 0.27879799666110183\n",
    "# 0 0.2988313856427379 0.27879799666110183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with rogue\n",
    "epsilons = [0.1, 0.02, 0.01, 0.005, 0.001, 0.0001, 0]\n",
    "for epsilon in epsilons:\n",
    "    confusion_matrix_overall, confusion_matrix_informative = test_rogue(dataset, epsilon=epsilon)\n",
    "    # print(confusion_matrix_overall)\n",
    "    print(epsilon, confusion_matrix_overall.get_accuracy(), confusion_matrix_informative.get_accuracy(), confusion_matrix_overall.get_accuracy_by_label(label=True), confusion_matrix_overall.get_accuracy_by_label(label=False), confusion_matrix_overall.get_accuracy_by_label(label=\"Equally Good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: check imbalance\n",
    "from collections import defaultdict\n",
    "dataset = json.load(open('data/pairwise_evaluation_w_embeddings.json'))\n",
    "label_counts = defaultdict(list)\n",
    "for datum in dataset:\n",
    "    label = datum['overall_writer_better']\n",
    "    label_counts[label].append(datum)\n",
    "print(len(label_counts[True]), len(label_counts[False]), len(label_counts['Equally Good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with wmd\n",
    "epsilons = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0]\n",
    "wmd_scorer = word_movers_distance.WordMoversDistance()\n",
    "for epsilon in epsilons:\n",
    "    confusion_matrix_overall, confusion_matrix_informative = test_wmd(dataset, wmd_scorer, epsilon=epsilon)\n",
    "    # print(confusion_matrix_overall)\n",
    "    print(epsilon, confusion_matrix_overall.get_accuracy(), confusion_matrix_informative.get_accuracy(), confusion_matrix_overall.get_accuracy_by_label(label=True), confusion_matrix_overall.get_accuracy_by_label(label=False), confusion_matrix_overall.get_accuracy_by_label(label=\"Equally Good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = open(\"api_key\").read()\n",
    "# test with llm evaluator\n",
    "confusion_matrix_overall, confusion_matrix_informative = test_llm(dataset, api_key)\n",
    "# print(confusion_matrix_overall)\n",
    "print(confusion_matrix_overall.get_accuracy(), confusion_matrix_informative.get_accuracy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
