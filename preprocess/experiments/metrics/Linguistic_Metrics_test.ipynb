{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stanza -U\n",
    "%pip install accelerate -U\n",
    "%pip install -U pip setuptools wheel textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stanford-corenlp: Package 'stanford-corenlp'\n",
      "[nltk_data]     not found in index\n",
      "[nltk_data] Error loading stanford-corenlp-4.2.0: Package 'stanford-\n",
      "[nltk_data]     corenlp-4.2.0' not found in index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7914fcbd3b8847b69c3bd0d14bd9861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 14:42:50 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-02-21 14:42:51 INFO: File exists: C:\\Users\\aryam\\stanza_resources\\en\\default.zip\n",
      "2024-02-21 14:42:55 INFO: Finished downloading models and saved to C:\\Users\\aryam\\stanza_resources.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Linguistic_Metrics as lm\n",
    "import json\n",
    "import timeit\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "\n",
    "def replace_punctuation_with_whitespace(input_string):\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    return input_string.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize class instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Laptop_Projects\\LLMEval-1\\preprocess\\experiments\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 15:13:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85282ca266474a6f90ab15871c097aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 15:13:59 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus                   |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-02-21 15:13:59 INFO: Using device: cpu\n",
      "2024-02-21 15:13:59 INFO: Loading: tokenize\n",
      "2024-02-21 15:13:59 INFO: Loading: mwt\n",
      "2024-02-21 15:13:59 INFO: Loading: pos\n",
      "2024-02-21 15:14:00 INFO: Loading: lemma\n",
      "2024-02-21 15:14:00 INFO: Loading: constituency\n",
      "2024-02-21 15:14:01 INFO: Loading: depparse\n",
      "2024-02-21 15:14:01 INFO: Loading: sentiment\n",
      "2024-02-21 15:14:01 INFO: Loading: ner\n",
      "2024-02-21 15:14:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "%cd D:\\Laptop_Projects\\LLMEval-1\\preprocess\\experiments\\data\n",
    "data = json.load(open(r'pairwise_evaluation_w_embeddings.json'))\n",
    "data_nat = json.load(open(r'final_metrics_naturalness.json'))\n",
    "df = pd.DataFrame(data_nat)\n",
    "nat = lm.naturalness(df)\n",
    "model = lm.LinguisticFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for calculating naturalness for any text \n",
    "\n",
    "Problem: If we use very few words in a sentence and use more number of sentences, the score exceeds 1 sometimes\n",
    "\n",
    "Potential Reason: The range for min max scaling is based on the dataset, for custom text the min max might change leading to score above 1 or less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7154226525399803"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = nat.calculate_naturalness('Evans said officers negotiated with the suspect while additional officers responded to the scene.During those negotiations, the man opened fired, killing Elmstrand, Ruge and Finseth, and injuring Medlicott.')\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all UPOS tags for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7120\\1050849109.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtags_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags_llm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspacy_tagger_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# save_json(tags_writer,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_writer.json')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# save_json(tags_llm,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_llm.json')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# save_json(tags_article,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_article.json')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Laptop_Projects\\LLMEval-1\\preprocess\\experiments\\metrics\\Linguistic_Metrics.py\u001b[0m in \u001b[0;36mspacy_tagger_dataset\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mdoc_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'writer_summary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mdoc_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'article_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mdoc_llm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text-davinci-002_summary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[0mtags_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_writer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mtags_llm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_llm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m                 \u001b[1;31m# This typically happens if a component is not initialized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nO\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malloc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mtokvecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokvecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \"\"\"\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     34\u001b[0m ) -> Tuple[SeqT, Callable]:\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ragged_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPadded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_padded_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\u001b[0m in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     89\u001b[0m ) -> Tuple[Ragged, Callable]:\n\u001b[0;32m     90\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mArrayXd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArrayXd\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataXd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdYr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\layers\\concatenate.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mdata_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_array_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\thinc\\layers\\concatenate.py\u001b[0m in \u001b[0;36m_array_forward\u001b[1;34m(model, X, Ys, callbacks, is_train)\u001b[0m\n\u001b[0;32m     71\u001b[0m ) -> Tuple[Array2d, Callable]:\n\u001b[0;32m     72\u001b[0m     \u001b[0mwidths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mArray2d\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tags_writer,tags_llm,tags_article = model.spacy_tagger_dataset(data)\n",
    "\n",
    "save_json(tags_writer,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_writer.json')\n",
    "save_json(tags_llm,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_llm.json')\n",
    "save_json(tags_article,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_article.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_writer = json.load(open(r'tags_writer.json'))\n",
    "tags_llm= json.load(open(r'tags_llm.json'))\n",
    "tags_article= json.load(open(r'tags_article.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate percentage of tags used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DET': 9.623379799461972, 'NOUN': 20.78136463683052, 'ADP': 11.65321594521888, 'PROPN': 9.280997798972853, 'VERB': 11.335289801907557, 'CCONJ': 2.3691611640988017, 'PRON': 4.01076057715823, 'PUNCT': 9.403277084861825, 'AUX': 5.5392516507703595, 'ADJ': 6.807899241868427, 'SCONJ': 1.7057960381511372, 'SPACE': 0.9384935191978478, 'NUM': 1.6966250917094645, 'ADV': 2.528124235754463, 'PART': 2.3080215211543167, 'X': 0.018341892883345562} {'DET': 10.157815631262524, 'NOUN': 20.052605210420843, 'ADP': 11.062750501002004, 'PROPN': 12.033441883767534, 'AUX': 5.573647294589178, 'VERB': 11.137900801603207, 'PART': 2.6928857715430863, 'PUNCT': 9.772670340681362, 'PRON': 3.547720440881764, 'ADJ': 5.92121743486974, 'ADV': 1.8881513026052104, 'CCONJ': 2.3327905811623246, 'NUM': 1.7566382765531061, 'SCONJ': 1.8474448897795592, 'SYM': 0.2035320641282565, 'X': 0.018787575150300603} {'PROPN': 9.368373536250905, 'PART': 2.6149282492669403, 'NOUN': 17.220934599292164, 'AUX': 4.8357300822013425, 'VERB': 10.824061816016174, 'DET': 8.13725006733151, 'PUNCT': 11.959214174895019, 'PRON': 6.262399429483994, 'ADP': 10.039412641633254, 'SPACE': 3.3806108011122027, 'ADJ': 5.648070525489244, 'SCONJ': 1.6518031568046554, 'NUM': 1.9268191836006994, 'CCONJ': 2.6318085433254557, 'ADV': 3.2421544565872975, 'INTJ': 0.03888157620219937, 'SYM': 0.1765792558255981, 'X': 0.040967904681341774}\n"
     ]
    }
   ],
   "source": [
    "tags_per_writer = model.count_tags_percentage(tags_writer)\n",
    "tags_per_llm = model.count_tags_percentage(tags_llm)\n",
    "tags_per_article = model.count_tags_percentage(tags_article)\n",
    "\n",
    "print(tags_per_writer,tags_per_llm,tags_per_article)\n",
    "\n",
    "save_json(tags_per_writer,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_percentage_writer.json')\n",
    "save_json(tags_per_llm,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_percentage_llm.json')\n",
    "save_json(tags_per_article,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/tags_percentage_article.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all writer and LLM summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_summs = []\n",
    "llm_summs = []\n",
    "article_summs = []\n",
    "rem_list = ['full_embedding', 'writer_summary_embedding', 'llm_summary_embedding']\n",
    "for key in rem_list:\n",
    "    for datum in data:\n",
    "        writer_summs.append(datum['writer_summary'])\n",
    "        llm_summs.append(datum['text-davinci-002_summary'])\n",
    "        article_summs.append(datum['article_text'])\n",
    "        del datum[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average dependency Tree heights for the whole dataset.(run the cell above before running this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8430239667400343 4.02686623246493 4.0256846002404965\n"
     ]
    }
   ],
   "source": [
    "tree_ht_article = model.calculate_average_tree_height(article_summs)\n",
    "tree_ht_writer = model.calculate_average_tree_height(writer_summs)\n",
    "tree_ht_llm = model.calculate_average_tree_height(llm_summs)\n",
    "print(tree_ht_writer,tree_ht_llm,tree_ht_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count dependency arcs based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number of sentences': 4788, 'average left arc length': 1.9764058198977585, 'average right arc length': 3.7244923697818137, 'average total arc length': 2.8673029952436044, 'right arc percentage': 50.964134207481685, 'left arc percentage': 49.035865792518315, 'standard deviation left arcs': 2.1279911149463357, 'standard deviation right arcs': 4.595775991690084, 'standard deviation total arcs': 3.7078829718281128} {'number of sentences': 4146, 'average left arc length': 1.8653553215444731, 'average right arc length': 4.3774183112261404, 'average total arc length': 3.1360214701839366, 'right arc percentage': 50.58257511291484, 'left arc percentage': 49.41742488708516, 'standard deviation left arcs': 1.7973213647715085, 'standard deviation right arcs': 5.41783435367229, 'standard deviation total arcs': 4.2451420578712495} {'number of sentences': 67140, 'average left arc length': 2.1942593671623642, 'average right arc length': 4.147497985255052, 'average total arc length': 3.2312057552360844, 'right arc percentage': 53.088566776663725, 'left arc percentage': 46.91143322333628, 'standard deviation left arcs': 2.883482320403541, 'standard deviation right arcs': 5.984221947285899, 'standard deviation total arcs': 4.884883130089451}\n"
     ]
    }
   ],
   "source": [
    "arcs_writer = model.count_dependency_arcs(writer_summs)\n",
    "arcs_llm = model.count_dependency_arcs(llm_summs)\n",
    "arcs_article = model.count_dependency_arcs(article_summs)\n",
    "\n",
    "print(arcs_writer,arcs_llm,arcs_article)\n",
    "\n",
    "# save_json(arcs_writer,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/arcs1_writer.json')\n",
    "# save_json(arcs_llm,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/arcs_llm1.json')\n",
    "# save_json(arcs_article,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/arcs_article1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average subtree analysis for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14812\\1587768311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_avg_llm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_subtree_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mllm_summs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_avg_llm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mword_avg_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_subtree_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_summs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_avg_article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mword_avg_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_subtree_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter_summs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Laptop_Projects\\LLMEval-1\\preprocess\\experiments\\metrics\\Linguistic_Metrics.py\u001b[0m in \u001b[0;36mcalculate_subtree_features\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_subtree_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0msubtree_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \"\"\"\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aryam\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE1041\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     def _ensure_doc_with_context(\n",
      "\u001b[1;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>"
     ]
    }
   ],
   "source": [
    "word_avg_llm = model.calculate_subtree_features(llm_summs)\n",
    "print(word_avg_llm)\n",
    "word_avg_article = model.calculate_subtree_features(article_summs)\n",
    "print(word_avg_article)\n",
    "word_avg_writer = model.calculate_subtree_features(writer_summs)\n",
    "print(word_avg_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average sentence length for all summaries in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.154671717171716 20.263386396526773 19.716268486916952\n"
     ]
    }
   ],
   "source": [
    "average_length_writer,average_length_llm,average_length_article = model.avg_sentence_length_dataset(data)\n",
    "print(average_length_writer,average_length_llm,average_length_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average word lengths for all summaries in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.858397518558533\n",
      "4.599166289651323\n",
      "4.809794325998426\n"
     ]
    }
   ],
   "source": [
    "word_avg_llm = model.average_word_length_system(llm_summs)\n",
    "print(word_avg_llm)\n",
    "word_avg_article = model.average_word_length_system(article_summs)\n",
    "print(word_avg_article)\n",
    "word_avg_writer = model.average_word_length_system(writer_summs)\n",
    "print(word_avg_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Gender Bias in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_ratio, llm_ratio, article_ratio = model.gender_ratio_dataset(data)\n",
    "print(writer_ratio, llm_ratio, article_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all analysis metrics for single data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating all the linguistic features for each data point in the dataset and creating a new document for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/599\n",
      "1/599\n",
      "2/599\n",
      "3/599\n",
      "4/599\n",
      "5/599\n",
      "6/599\n",
      "7/599\n",
      "8/599\n",
      "9/599\n",
      "10/599\n",
      "11/599\n",
      "12/599\n",
      "13/599\n",
      "14/599\n",
      "15/599\n",
      "16/599\n",
      "17/599\n",
      "18/599\n",
      "19/599\n",
      "20/599\n",
      "21/599\n",
      "22/599\n",
      "23/599\n",
      "24/599\n",
      "25/599\n",
      "26/599\n",
      "27/599\n",
      "28/599\n",
      "29/599\n",
      "30/599\n",
      "31/599\n",
      "32/599\n",
      "33/599\n",
      "34/599\n",
      "35/599\n",
      "36/599\n",
      "37/599\n",
      "38/599\n",
      "39/599\n",
      "40/599\n",
      "41/599\n",
      "42/599\n",
      "43/599\n",
      "44/599\n",
      "45/599\n",
      "46/599\n",
      "47/599\n",
      "48/599\n",
      "49/599\n",
      "50/599\n",
      "51/599\n",
      "52/599\n",
      "53/599\n",
      "54/599\n",
      "55/599\n",
      "56/599\n",
      "57/599\n",
      "58/599\n",
      "59/599\n",
      "60/599\n",
      "61/599\n",
      "62/599\n",
      "63/599\n",
      "64/599\n",
      "65/599\n",
      "66/599\n",
      "67/599\n",
      "68/599\n",
      "69/599\n",
      "70/599\n",
      "71/599\n",
      "72/599\n",
      "73/599\n",
      "74/599\n",
      "75/599\n",
      "76/599\n",
      "77/599\n",
      "78/599\n",
      "79/599\n",
      "80/599\n",
      "81/599\n",
      "82/599\n",
      "83/599\n",
      "84/599\n",
      "85/599\n",
      "86/599\n",
      "87/599\n",
      "88/599\n",
      "89/599\n",
      "90/599\n",
      "91/599\n",
      "92/599\n",
      "93/599\n",
      "94/599\n",
      "95/599\n",
      "96/599\n",
      "97/599\n",
      "98/599\n",
      "99/599\n",
      "100/599\n",
      "101/599\n",
      "102/599\n",
      "103/599\n",
      "104/599\n",
      "105/599\n",
      "106/599\n",
      "107/599\n",
      "108/599\n",
      "109/599\n",
      "110/599\n",
      "111/599\n",
      "112/599\n",
      "113/599\n",
      "114/599\n",
      "115/599\n",
      "116/599\n",
      "117/599\n",
      "118/599\n",
      "119/599\n",
      "120/599\n",
      "121/599\n",
      "122/599\n",
      "123/599\n",
      "124/599\n",
      "125/599\n",
      "126/599\n",
      "127/599\n",
      "128/599\n",
      "129/599\n",
      "130/599\n",
      "131/599\n",
      "132/599\n",
      "133/599\n",
      "134/599\n",
      "135/599\n",
      "136/599\n",
      "137/599\n",
      "138/599\n",
      "139/599\n",
      "140/599\n",
      "141/599\n",
      "142/599\n",
      "143/599\n",
      "144/599\n",
      "145/599\n",
      "146/599\n",
      "147/599\n",
      "148/599\n",
      "149/599\n",
      "150/599\n",
      "151/599\n",
      "152/599\n",
      "153/599\n",
      "154/599\n",
      "155/599\n",
      "156/599\n",
      "157/599\n",
      "158/599\n",
      "159/599\n",
      "160/599\n",
      "161/599\n",
      "162/599\n",
      "163/599\n",
      "164/599\n",
      "165/599\n",
      "166/599\n",
      "167/599\n",
      "168/599\n",
      "169/599\n",
      "170/599\n",
      "171/599\n",
      "172/599\n",
      "173/599\n",
      "174/599\n",
      "175/599\n",
      "176/599\n",
      "177/599\n",
      "178/599\n",
      "179/599\n",
      "180/599\n",
      "181/599\n",
      "182/599\n",
      "183/599\n",
      "184/599\n",
      "185/599\n",
      "186/599\n",
      "187/599\n",
      "188/599\n",
      "189/599\n",
      "190/599\n",
      "191/599\n",
      "192/599\n",
      "193/599\n",
      "194/599\n",
      "195/599\n",
      "196/599\n",
      "197/599\n",
      "198/599\n",
      "199/599\n",
      "200/599\n",
      "201/599\n",
      "202/599\n",
      "203/599\n",
      "204/599\n",
      "205/599\n",
      "206/599\n",
      "207/599\n",
      "208/599\n",
      "209/599\n",
      "210/599\n",
      "211/599\n",
      "212/599\n",
      "213/599\n",
      "214/599\n",
      "215/599\n",
      "216/599\n",
      "217/599\n",
      "218/599\n",
      "219/599\n",
      "220/599\n",
      "221/599\n",
      "222/599\n",
      "223/599\n",
      "224/599\n",
      "225/599\n",
      "226/599\n",
      "227/599\n",
      "228/599\n",
      "229/599\n",
      "230/599\n",
      "231/599\n",
      "232/599\n",
      "233/599\n",
      "234/599\n",
      "235/599\n",
      "236/599\n",
      "237/599\n",
      "238/599\n",
      "239/599\n",
      "240/599\n",
      "241/599\n",
      "242/599\n",
      "243/599\n",
      "244/599\n",
      "245/599\n",
      "246/599\n",
      "247/599\n",
      "248/599\n",
      "249/599\n",
      "250/599\n",
      "251/599\n",
      "252/599\n",
      "253/599\n",
      "254/599\n",
      "255/599\n",
      "256/599\n",
      "257/599\n",
      "258/599\n",
      "259/599\n",
      "260/599\n",
      "261/599\n",
      "262/599\n",
      "263/599\n",
      "264/599\n",
      "265/599\n",
      "266/599\n",
      "267/599\n",
      "268/599\n",
      "269/599\n",
      "270/599\n",
      "271/599\n",
      "272/599\n",
      "273/599\n",
      "274/599\n",
      "275/599\n",
      "276/599\n",
      "277/599\n",
      "278/599\n",
      "279/599\n",
      "280/599\n",
      "281/599\n",
      "282/599\n",
      "283/599\n",
      "284/599\n",
      "285/599\n",
      "286/599\n",
      "287/599\n",
      "288/599\n",
      "289/599\n",
      "290/599\n",
      "291/599\n",
      "292/599\n",
      "293/599\n",
      "294/599\n",
      "295/599\n",
      "296/599\n",
      "297/599\n",
      "298/599\n",
      "299/599\n",
      "300/599\n",
      "301/599\n",
      "302/599\n",
      "303/599\n",
      "304/599\n",
      "305/599\n",
      "306/599\n",
      "307/599\n",
      "308/599\n",
      "309/599\n",
      "310/599\n",
      "311/599\n",
      "312/599\n",
      "313/599\n",
      "314/599\n",
      "315/599\n",
      "316/599\n",
      "317/599\n",
      "318/599\n",
      "319/599\n",
      "320/599\n",
      "321/599\n",
      "322/599\n",
      "323/599\n",
      "324/599\n",
      "325/599\n",
      "326/599\n",
      "327/599\n",
      "328/599\n",
      "329/599\n",
      "330/599\n",
      "331/599\n",
      "332/599\n",
      "333/599\n",
      "334/599\n",
      "335/599\n",
      "336/599\n",
      "337/599\n",
      "338/599\n",
      "339/599\n",
      "340/599\n",
      "341/599\n",
      "342/599\n",
      "343/599\n",
      "344/599\n",
      "345/599\n",
      "346/599\n",
      "347/599\n",
      "348/599\n",
      "349/599\n",
      "350/599\n",
      "351/599\n",
      "352/599\n",
      "353/599\n",
      "354/599\n",
      "355/599\n",
      "356/599\n",
      "357/599\n",
      "358/599\n",
      "359/599\n",
      "360/599\n",
      "361/599\n",
      "362/599\n",
      "363/599\n",
      "364/599\n",
      "365/599\n",
      "366/599\n",
      "367/599\n",
      "368/599\n",
      "369/599\n",
      "370/599\n",
      "371/599\n",
      "372/599\n",
      "373/599\n",
      "374/599\n",
      "375/599\n",
      "376/599\n",
      "377/599\n",
      "378/599\n",
      "379/599\n",
      "380/599\n",
      "381/599\n",
      "382/599\n",
      "383/599\n",
      "384/599\n",
      "385/599\n",
      "386/599\n",
      "387/599\n",
      "388/599\n",
      "389/599\n",
      "390/599\n",
      "391/599\n",
      "392/599\n",
      "393/599\n",
      "394/599\n",
      "395/599\n",
      "396/599\n",
      "397/599\n",
      "398/599\n",
      "399/599\n",
      "400/599\n",
      "401/599\n",
      "402/599\n",
      "403/599\n",
      "404/599\n",
      "405/599\n",
      "406/599\n",
      "407/599\n",
      "408/599\n",
      "409/599\n",
      "410/599\n",
      "411/599\n",
      "412/599\n",
      "413/599\n",
      "414/599\n",
      "415/599\n",
      "416/599\n",
      "417/599\n",
      "418/599\n",
      "419/599\n",
      "420/599\n",
      "421/599\n",
      "422/599\n",
      "423/599\n",
      "424/599\n",
      "425/599\n",
      "426/599\n",
      "427/599\n",
      "428/599\n",
      "429/599\n",
      "430/599\n",
      "431/599\n",
      "432/599\n",
      "433/599\n",
      "434/599\n",
      "435/599\n",
      "436/599\n",
      "437/599\n",
      "438/599\n",
      "439/599\n",
      "440/599\n",
      "441/599\n",
      "442/599\n",
      "443/599\n",
      "444/599\n",
      "445/599\n",
      "446/599\n",
      "447/599\n",
      "448/599\n",
      "449/599\n",
      "450/599\n",
      "451/599\n",
      "452/599\n",
      "453/599\n",
      "454/599\n",
      "455/599\n",
      "456/599\n",
      "457/599\n",
      "458/599\n",
      "459/599\n",
      "460/599\n",
      "461/599\n",
      "462/599\n",
      "463/599\n",
      "464/599\n",
      "465/599\n",
      "466/599\n",
      "467/599\n",
      "468/599\n",
      "469/599\n",
      "470/599\n",
      "471/599\n",
      "472/599\n",
      "473/599\n",
      "474/599\n",
      "475/599\n",
      "476/599\n",
      "477/599\n",
      "478/599\n",
      "479/599\n",
      "480/599\n",
      "481/599\n",
      "482/599\n",
      "483/599\n",
      "484/599\n",
      "485/599\n",
      "486/599\n",
      "487/599\n",
      "488/599\n",
      "489/599\n",
      "490/599\n",
      "491/599\n",
      "492/599\n",
      "493/599\n",
      "494/599\n",
      "495/599\n",
      "496/599\n",
      "497/599\n",
      "498/599\n",
      "499/599\n",
      "500/599\n",
      "501/599\n",
      "502/599\n",
      "503/599\n",
      "504/599\n",
      "505/599\n",
      "506/599\n",
      "507/599\n",
      "508/599\n",
      "509/599\n",
      "510/599\n",
      "511/599\n",
      "512/599\n",
      "513/599\n",
      "514/599\n",
      "515/599\n",
      "516/599\n",
      "517/599\n",
      "518/599\n",
      "519/599\n",
      "520/599\n",
      "521/599\n",
      "522/599\n",
      "523/599\n",
      "524/599\n",
      "525/599\n",
      "526/599\n",
      "527/599\n",
      "528/599\n",
      "529/599\n",
      "530/599\n",
      "531/599\n",
      "532/599\n",
      "533/599\n",
      "534/599\n",
      "535/599\n",
      "536/599\n",
      "537/599\n",
      "538/599\n",
      "539/599\n",
      "540/599\n",
      "541/599\n",
      "542/599\n",
      "543/599\n",
      "544/599\n",
      "545/599\n",
      "546/599\n",
      "547/599\n",
      "548/599\n",
      "549/599\n",
      "550/599\n",
      "551/599\n",
      "552/599\n",
      "553/599\n",
      "554/599\n",
      "555/599\n",
      "556/599\n",
      "557/599\n",
      "558/599\n",
      "559/599\n",
      "560/599\n",
      "561/599\n",
      "562/599\n",
      "563/599\n",
      "564/599\n",
      "565/599\n",
      "566/599\n",
      "567/599\n",
      "568/599\n",
      "569/599\n",
      "570/599\n",
      "571/599\n",
      "572/599\n",
      "573/599\n",
      "574/599\n",
      "575/599\n",
      "576/599\n",
      "577/599\n",
      "578/599\n",
      "579/599\n",
      "580/599\n",
      "581/599\n",
      "582/599\n",
      "583/599\n",
      "584/599\n",
      "585/599\n",
      "586/599\n",
      "587/599\n",
      "588/599\n",
      "589/599\n",
      "590/599\n",
      "591/599\n",
      "592/599\n",
      "593/599\n",
      "594/599\n",
      "595/599\n",
      "596/599\n",
      "597/599\n",
      "598/599\n"
     ]
    }
   ],
   "source": [
    "for i,datum in enumerate(data):\n",
    "    print('{}/{}'.format(i, len(data)))\n",
    "    avg_len_writer = model.average_word_length_system([datum['writer_summary']])\n",
    "    avg_len_llm = model.average_word_length_system([datum['text-davinci-002_summary']])\n",
    "    avg_len_article = model.average_word_length_system([datum['article_text']])\n",
    "    tree_ht_article = model.calculate_average_tree_height([datum['article_text']])\n",
    "    tree_ht_llm = model.calculate_average_tree_height([datum['text-davinci-002_summary']])\n",
    "    tree_ht_writer = model.calculate_average_tree_height([datum['writer_summary']])\n",
    "    tags_writer = model.spacy_tagger(datum['writer_summary'])\n",
    "    tags_llm = model.spacy_tagger(datum['text-davinci-002_summary'])\n",
    "    tags_article = model.spacy_tagger(datum['article_text'])\n",
    "    tags_per_writer = model.count_tags_percentage([tags_writer])\n",
    "    tags_per_llm = model.count_tags_percentage([tags_llm])\n",
    "    tags_per_article = model.count_tags_percentage([tags_article])\n",
    "    sen_len_writer = model.avg_sentence_length(datum['writer_summary'])\n",
    "    sen_len_llm = model.avg_sentence_length(datum['text-davinci-002_summary'])\n",
    "    sen_len_article = model.avg_sentence_length(datum['article_text'])\n",
    "    arcs_writer = model.count_dependency_arcs([datum['writer_summary']])\n",
    "    arcs_llm = model.count_dependency_arcs([datum['text-davinci-002_summary']])\n",
    "    arcs_article = model.count_dependency_arcs([datum['article_text']])\n",
    "    subtree_writer = model.calculate_subtree_features(datum['writer_summary'])\n",
    "    subtree_llm = model.calculate_subtree_features(datum['text-davinci-002_summary'])\n",
    "    subtree_article = model.calculate_subtree_features(datum['article_text'])\n",
    "    gender_writer = model.gender_ratio_single(datum['writer_summary'])\n",
    "    gender_llm = model.gender_ratio_single(datum['text-davinci-002_summary'])\n",
    "    gender_article = model.gender_ratio_single(datum['article_text'])\n",
    "\n",
    "    datum['average word lengths'] = {\"writer\":avg_len_writer,\"LLM\":avg_len_llm,\"article\":avg_len_article}\n",
    "    datum['Average Dependency tree heights'] = {\"writer\":tree_ht_writer,\"LLM\":tree_ht_llm,\"article\":tree_ht_article}\n",
    "    datum['Percentage of UPOS tags used'] = {\"writer\":tags_per_writer,\"LLM\":tags_per_llm,\"article\":tags_per_article}\n",
    "    datum['average sentence lengths'] = {\"writer\":sen_len_writer,\"LLM\":sen_len_llm,\"article\":sen_len_article}\n",
    "    datum['Arc analysis'] = {\"writer\":arcs_writer,\"LLM\":arcs_llm,\"article\":arcs_article}\n",
    "    datum['Gender Ratios'] = {\"writer\":gender_writer,\"LLM\":gender_llm,\"article\":gender_article}\n",
    "    datum['Left and right subtrees analysis'] = {\"writer\":model.calculate_subtree_features(datum['writer_summary']),\"LLM\":model.calculate_subtree_features(datum['text-davinci-002_summary']),\"article\":model.calculate_subtree_features(datum['article_text'])}\n",
    "\n",
    "\n",
    "save_json(data,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/data_analysis.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = json.load(open(r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/data_analysis.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Delta values document for calculating naturalness(Not used in actual calculation, but used in analysis of metrics to be selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/599\n",
      "1/599\n",
      "2/599\n",
      "3/599\n",
      "4/599\n",
      "5/599\n",
      "6/599\n",
      "7/599\n",
      "8/599\n",
      "9/599\n",
      "10/599\n",
      "11/599\n",
      "12/599\n",
      "13/599\n",
      "14/599\n",
      "15/599\n",
      "16/599\n",
      "17/599\n",
      "18/599\n",
      "19/599\n",
      "20/599\n",
      "21/599\n",
      "22/599\n",
      "23/599\n",
      "24/599\n",
      "25/599\n",
      "26/599\n",
      "27/599\n",
      "28/599\n",
      "29/599\n",
      "30/599\n",
      "31/599\n",
      "32/599\n",
      "33/599\n",
      "34/599\n",
      "35/599\n",
      "36/599\n",
      "37/599\n",
      "38/599\n",
      "39/599\n",
      "40/599\n",
      "41/599\n",
      "42/599\n",
      "43/599\n",
      "44/599\n",
      "45/599\n",
      "46/599\n",
      "47/599\n",
      "48/599\n",
      "49/599\n",
      "50/599\n",
      "51/599\n",
      "52/599\n",
      "53/599\n",
      "54/599\n",
      "55/599\n",
      "56/599\n",
      "57/599\n",
      "58/599\n",
      "59/599\n",
      "60/599\n",
      "61/599\n",
      "62/599\n",
      "63/599\n",
      "64/599\n",
      "65/599\n",
      "66/599\n",
      "67/599\n",
      "68/599\n",
      "69/599\n",
      "70/599\n",
      "71/599\n",
      "72/599\n",
      "73/599\n",
      "74/599\n",
      "75/599\n",
      "76/599\n",
      "77/599\n",
      "78/599\n",
      "79/599\n",
      "80/599\n",
      "81/599\n",
      "82/599\n",
      "83/599\n",
      "84/599\n",
      "85/599\n",
      "86/599\n",
      "87/599\n",
      "88/599\n",
      "89/599\n",
      "90/599\n",
      "91/599\n",
      "92/599\n",
      "93/599\n",
      "94/599\n",
      "95/599\n",
      "96/599\n",
      "97/599\n",
      "98/599\n",
      "99/599\n",
      "100/599\n",
      "101/599\n",
      "102/599\n",
      "103/599\n",
      "104/599\n",
      "105/599\n",
      "106/599\n",
      "107/599\n",
      "108/599\n",
      "109/599\n",
      "110/599\n",
      "111/599\n",
      "112/599\n",
      "113/599\n",
      "114/599\n",
      "115/599\n",
      "116/599\n",
      "117/599\n",
      "118/599\n",
      "119/599\n",
      "120/599\n",
      "121/599\n",
      "122/599\n",
      "123/599\n",
      "124/599\n",
      "125/599\n",
      "126/599\n",
      "127/599\n",
      "128/599\n",
      "129/599\n",
      "130/599\n",
      "131/599\n",
      "132/599\n",
      "133/599\n",
      "134/599\n",
      "135/599\n",
      "136/599\n",
      "137/599\n",
      "138/599\n",
      "139/599\n",
      "140/599\n",
      "141/599\n",
      "142/599\n",
      "143/599\n",
      "144/599\n",
      "145/599\n",
      "146/599\n",
      "147/599\n",
      "148/599\n",
      "149/599\n",
      "150/599\n",
      "151/599\n",
      "152/599\n",
      "153/599\n",
      "154/599\n",
      "155/599\n",
      "156/599\n",
      "157/599\n",
      "158/599\n",
      "159/599\n",
      "160/599\n",
      "161/599\n",
      "162/599\n",
      "163/599\n",
      "164/599\n",
      "165/599\n",
      "166/599\n",
      "167/599\n",
      "168/599\n",
      "169/599\n",
      "170/599\n",
      "171/599\n",
      "172/599\n",
      "173/599\n",
      "174/599\n",
      "175/599\n",
      "176/599\n",
      "177/599\n",
      "178/599\n",
      "179/599\n",
      "180/599\n",
      "181/599\n",
      "182/599\n",
      "183/599\n",
      "184/599\n",
      "185/599\n",
      "186/599\n",
      "187/599\n",
      "188/599\n",
      "189/599\n",
      "190/599\n",
      "191/599\n",
      "192/599\n",
      "193/599\n",
      "194/599\n",
      "195/599\n",
      "196/599\n",
      "197/599\n",
      "198/599\n",
      "199/599\n",
      "200/599\n",
      "201/599\n",
      "202/599\n",
      "203/599\n",
      "204/599\n",
      "205/599\n",
      "206/599\n",
      "207/599\n",
      "208/599\n",
      "209/599\n",
      "210/599\n",
      "211/599\n",
      "212/599\n",
      "213/599\n",
      "214/599\n",
      "215/599\n",
      "216/599\n",
      "217/599\n",
      "218/599\n",
      "219/599\n",
      "220/599\n",
      "221/599\n",
      "222/599\n",
      "223/599\n",
      "224/599\n",
      "225/599\n",
      "226/599\n",
      "227/599\n",
      "228/599\n",
      "229/599\n",
      "230/599\n",
      "231/599\n",
      "232/599\n",
      "233/599\n",
      "234/599\n",
      "235/599\n",
      "236/599\n",
      "237/599\n",
      "238/599\n",
      "239/599\n",
      "240/599\n",
      "241/599\n",
      "242/599\n",
      "243/599\n",
      "244/599\n",
      "245/599\n",
      "246/599\n",
      "247/599\n",
      "248/599\n",
      "249/599\n",
      "250/599\n",
      "251/599\n",
      "252/599\n",
      "253/599\n",
      "254/599\n",
      "255/599\n",
      "256/599\n",
      "257/599\n",
      "258/599\n",
      "259/599\n",
      "260/599\n",
      "261/599\n",
      "262/599\n",
      "263/599\n",
      "264/599\n",
      "265/599\n",
      "266/599\n",
      "267/599\n",
      "268/599\n",
      "269/599\n",
      "270/599\n",
      "271/599\n",
      "272/599\n",
      "273/599\n",
      "274/599\n",
      "275/599\n",
      "276/599\n",
      "277/599\n",
      "278/599\n",
      "279/599\n",
      "280/599\n",
      "281/599\n",
      "282/599\n",
      "283/599\n",
      "284/599\n",
      "285/599\n",
      "286/599\n",
      "287/599\n",
      "288/599\n",
      "289/599\n",
      "290/599\n",
      "291/599\n",
      "292/599\n",
      "293/599\n",
      "294/599\n",
      "295/599\n",
      "296/599\n",
      "297/599\n",
      "298/599\n",
      "299/599\n",
      "300/599\n",
      "301/599\n",
      "302/599\n",
      "303/599\n",
      "304/599\n",
      "305/599\n",
      "306/599\n",
      "307/599\n",
      "308/599\n",
      "309/599\n",
      "310/599\n",
      "311/599\n",
      "312/599\n",
      "313/599\n",
      "314/599\n",
      "315/599\n",
      "316/599\n",
      "317/599\n",
      "318/599\n",
      "319/599\n",
      "320/599\n",
      "321/599\n",
      "322/599\n",
      "323/599\n",
      "324/599\n",
      "325/599\n",
      "326/599\n",
      "327/599\n",
      "328/599\n",
      "329/599\n",
      "330/599\n",
      "331/599\n",
      "332/599\n",
      "333/599\n",
      "334/599\n",
      "335/599\n",
      "336/599\n",
      "337/599\n",
      "338/599\n",
      "339/599\n",
      "340/599\n",
      "341/599\n",
      "342/599\n",
      "343/599\n",
      "344/599\n",
      "345/599\n",
      "346/599\n",
      "347/599\n",
      "348/599\n",
      "349/599\n",
      "350/599\n",
      "351/599\n",
      "352/599\n",
      "353/599\n",
      "354/599\n",
      "355/599\n",
      "356/599\n",
      "357/599\n",
      "358/599\n",
      "359/599\n",
      "360/599\n",
      "361/599\n",
      "362/599\n",
      "363/599\n",
      "364/599\n",
      "365/599\n",
      "366/599\n",
      "367/599\n",
      "368/599\n",
      "369/599\n",
      "370/599\n",
      "371/599\n",
      "372/599\n",
      "373/599\n",
      "374/599\n",
      "375/599\n",
      "376/599\n",
      "377/599\n",
      "378/599\n",
      "379/599\n",
      "380/599\n",
      "381/599\n",
      "382/599\n",
      "383/599\n",
      "384/599\n",
      "385/599\n",
      "386/599\n",
      "387/599\n",
      "388/599\n",
      "389/599\n",
      "390/599\n",
      "391/599\n",
      "392/599\n",
      "393/599\n",
      "394/599\n",
      "395/599\n",
      "396/599\n",
      "397/599\n",
      "398/599\n",
      "399/599\n",
      "400/599\n",
      "401/599\n",
      "402/599\n",
      "403/599\n",
      "404/599\n",
      "405/599\n",
      "406/599\n",
      "407/599\n",
      "408/599\n",
      "409/599\n",
      "410/599\n",
      "411/599\n",
      "412/599\n",
      "413/599\n",
      "414/599\n",
      "415/599\n",
      "416/599\n",
      "417/599\n",
      "418/599\n",
      "419/599\n",
      "420/599\n",
      "421/599\n",
      "422/599\n",
      "423/599\n",
      "424/599\n",
      "425/599\n",
      "426/599\n",
      "427/599\n",
      "428/599\n",
      "429/599\n",
      "430/599\n",
      "431/599\n",
      "432/599\n",
      "433/599\n",
      "434/599\n",
      "435/599\n",
      "436/599\n",
      "437/599\n",
      "438/599\n",
      "439/599\n",
      "440/599\n",
      "441/599\n",
      "442/599\n",
      "443/599\n",
      "444/599\n",
      "445/599\n",
      "446/599\n",
      "447/599\n",
      "448/599\n",
      "449/599\n",
      "450/599\n",
      "451/599\n",
      "452/599\n",
      "453/599\n",
      "454/599\n",
      "455/599\n",
      "456/599\n",
      "457/599\n",
      "458/599\n",
      "459/599\n",
      "460/599\n",
      "461/599\n",
      "462/599\n",
      "463/599\n",
      "464/599\n",
      "465/599\n",
      "466/599\n",
      "467/599\n",
      "468/599\n",
      "469/599\n",
      "470/599\n",
      "471/599\n",
      "472/599\n",
      "473/599\n",
      "474/599\n",
      "475/599\n",
      "476/599\n",
      "477/599\n",
      "478/599\n",
      "479/599\n",
      "480/599\n",
      "481/599\n",
      "482/599\n",
      "483/599\n",
      "484/599\n",
      "485/599\n",
      "486/599\n",
      "487/599\n",
      "488/599\n",
      "489/599\n",
      "490/599\n",
      "491/599\n",
      "492/599\n",
      "493/599\n",
      "494/599\n",
      "495/599\n",
      "496/599\n",
      "497/599\n",
      "498/599\n",
      "499/599\n",
      "500/599\n",
      "501/599\n",
      "502/599\n",
      "503/599\n",
      "504/599\n",
      "505/599\n",
      "506/599\n",
      "507/599\n",
      "508/599\n",
      "509/599\n",
      "510/599\n",
      "511/599\n",
      "512/599\n",
      "513/599\n",
      "514/599\n",
      "515/599\n",
      "516/599\n",
      "517/599\n",
      "518/599\n",
      "519/599\n",
      "520/599\n",
      "521/599\n",
      "522/599\n",
      "523/599\n",
      "524/599\n",
      "525/599\n",
      "526/599\n",
      "527/599\n",
      "528/599\n",
      "529/599\n",
      "530/599\n",
      "531/599\n",
      "532/599\n",
      "533/599\n",
      "534/599\n",
      "535/599\n",
      "536/599\n",
      "537/599\n",
      "538/599\n",
      "539/599\n",
      "540/599\n",
      "541/599\n",
      "542/599\n",
      "543/599\n",
      "544/599\n",
      "545/599\n",
      "546/599\n",
      "547/599\n",
      "548/599\n",
      "549/599\n",
      "550/599\n",
      "551/599\n",
      "552/599\n",
      "553/599\n",
      "554/599\n",
      "555/599\n",
      "556/599\n",
      "557/599\n",
      "558/599\n",
      "559/599\n",
      "560/599\n",
      "561/599\n",
      "562/599\n",
      "563/599\n",
      "564/599\n",
      "565/599\n",
      "566/599\n",
      "567/599\n",
      "568/599\n",
      "569/599\n",
      "570/599\n",
      "571/599\n",
      "572/599\n",
      "573/599\n",
      "574/599\n",
      "575/599\n",
      "576/599\n",
      "577/599\n",
      "578/599\n",
      "579/599\n",
      "580/599\n",
      "581/599\n",
      "582/599\n",
      "583/599\n",
      "584/599\n",
      "585/599\n",
      "586/599\n",
      "587/599\n",
      "588/599\n",
      "589/599\n",
      "590/599\n",
      "591/599\n",
      "592/599\n",
      "593/599\n",
      "594/599\n",
      "595/599\n",
      "596/599\n",
      "597/599\n",
      "598/599\n"
     ]
    }
   ],
   "source": [
    "list_articles = []\n",
    "for i,datum in enumerate(analysis_data):\n",
    "    print('{}/{}'.format(i, len(data)))\n",
    "    natural_dict = {}\n",
    "    natural_dict['article_id'] = datum['article_id']\n",
    "    natural_dict['writer_summary'] = datum['writer_summary']\n",
    "    natural_dict['text-davinci-002_summary'] = datum['text-davinci-002_summary']\n",
    "    natural_dict['overall_writer_better'] = datum['overall_writer_better']\n",
    "    natural_dict['informative_writer_better'] = datum['informative_writer_better']\n",
    "    natural_dict['delta average word lengths'] = datum['average word lengths']['writer'] - datum['average word lengths']['LLM']\n",
    "    natural_dict['delta Average Dependency tree heights'] = datum['Average Dependency tree heights']['writer'] - datum['Average Dependency tree heights']['LLM']\n",
    "    natural_dict['delta_average_sentence_lengths'] = datum['average sentence lengths']['writer'] - datum['average sentence lengths']['LLM']\n",
    "    natural_dict['delta_average_left_arc_length'] = datum['Arc analysis']['writer']['average left arc length'] - datum['Arc analysis']['LLM']['average left arc length']\n",
    "    natural_dict['delta_average_right_arc_length'] = datum['Arc analysis']['writer']['average right arc length'] - datum['Arc analysis']['LLM']['average right arc length']\n",
    "    natural_dict['delta_average_right_arc_length'] = datum['Arc analysis']['writer']['average total arc length'] - datum['Arc analysis']['LLM']['average total arc length']\n",
    "    natural_dict['delta_avg_left_subtree_height'] = datum['Left and right subtrees analysis']['writer']['avg_left_subtree_height'] - datum['Left and right subtrees analysis']['LLM']['avg_left_subtree_height']\n",
    "    natural_dict['delta_avg_right_subtree_height'] = datum['Left and right subtrees analysis']['writer']['avg_right_subtree_height'] - datum['Left and right subtrees analysis']['LLM']['avg_right_subtree_height']\n",
    "    natural_dict['delta_num_left_subtrees'] = datum['Left and right subtrees analysis']['writer']['num_left_subtrees'] - datum['Left and right subtrees analysis']['LLM']['num_left_subtrees']\n",
    "    natural_dict['delta_num_right_subtrees'] = datum['Left and right subtrees analysis']['writer']['num_right_subtrees'] - datum['Left and right subtrees analysis']['LLM']['num_right_subtrees']\n",
    "    natural_dict['delta_no_sentences'] = datum['Arc analysis']['writer']['number of sentences'] - datum['Arc analysis']['LLM']['number of sentences']\n",
    "    natural_dict['no_sentences_writer'] = datum['Arc analysis']['writer']['number of sentences']\n",
    "    natural_dict['no_sentences_llm'] = datum['Arc analysis']['LLM']['number of sentences']\n",
    "    list_articles.append(natural_dict)\n",
    "save_json(list_articles,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/delta_metrics_natural.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document for the final metrics used for calculating naturalness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/599\n",
      "1/599\n",
      "2/599\n",
      "3/599\n",
      "4/599\n",
      "5/599\n",
      "6/599\n",
      "7/599\n",
      "8/599\n",
      "9/599\n",
      "10/599\n",
      "11/599\n",
      "12/599\n",
      "13/599\n",
      "14/599\n",
      "15/599\n",
      "16/599\n",
      "17/599\n",
      "18/599\n",
      "19/599\n",
      "20/599\n",
      "21/599\n",
      "22/599\n",
      "23/599\n",
      "24/599\n",
      "25/599\n",
      "26/599\n",
      "27/599\n",
      "28/599\n",
      "29/599\n",
      "30/599\n",
      "31/599\n",
      "32/599\n",
      "33/599\n",
      "34/599\n",
      "35/599\n",
      "36/599\n",
      "37/599\n",
      "38/599\n",
      "39/599\n",
      "40/599\n",
      "41/599\n",
      "42/599\n",
      "43/599\n",
      "44/599\n",
      "45/599\n",
      "46/599\n",
      "47/599\n",
      "48/599\n",
      "49/599\n",
      "50/599\n",
      "51/599\n",
      "52/599\n",
      "53/599\n",
      "54/599\n",
      "55/599\n",
      "56/599\n",
      "57/599\n",
      "58/599\n",
      "59/599\n",
      "60/599\n",
      "61/599\n",
      "62/599\n",
      "63/599\n",
      "64/599\n",
      "65/599\n",
      "66/599\n",
      "67/599\n",
      "68/599\n",
      "69/599\n",
      "70/599\n",
      "71/599\n",
      "72/599\n",
      "73/599\n",
      "74/599\n",
      "75/599\n",
      "76/599\n",
      "77/599\n",
      "78/599\n",
      "79/599\n",
      "80/599\n",
      "81/599\n",
      "82/599\n",
      "83/599\n",
      "84/599\n",
      "85/599\n",
      "86/599\n",
      "87/599\n",
      "88/599\n",
      "89/599\n",
      "90/599\n",
      "91/599\n",
      "92/599\n",
      "93/599\n",
      "94/599\n",
      "95/599\n",
      "96/599\n",
      "97/599\n",
      "98/599\n",
      "99/599\n",
      "100/599\n",
      "101/599\n",
      "102/599\n",
      "103/599\n",
      "104/599\n",
      "105/599\n",
      "106/599\n",
      "107/599\n",
      "108/599\n",
      "109/599\n",
      "110/599\n",
      "111/599\n",
      "112/599\n",
      "113/599\n",
      "114/599\n",
      "115/599\n",
      "116/599\n",
      "117/599\n",
      "118/599\n",
      "119/599\n",
      "120/599\n",
      "121/599\n",
      "122/599\n",
      "123/599\n",
      "124/599\n",
      "125/599\n",
      "126/599\n",
      "127/599\n",
      "128/599\n",
      "129/599\n",
      "130/599\n",
      "131/599\n",
      "132/599\n",
      "133/599\n",
      "134/599\n",
      "135/599\n",
      "136/599\n",
      "137/599\n",
      "138/599\n",
      "139/599\n",
      "140/599\n",
      "141/599\n",
      "142/599\n",
      "143/599\n",
      "144/599\n",
      "145/599\n",
      "146/599\n",
      "147/599\n",
      "148/599\n",
      "149/599\n",
      "150/599\n",
      "151/599\n",
      "152/599\n",
      "153/599\n",
      "154/599\n",
      "155/599\n",
      "156/599\n",
      "157/599\n",
      "158/599\n",
      "159/599\n",
      "160/599\n",
      "161/599\n",
      "162/599\n",
      "163/599\n",
      "164/599\n",
      "165/599\n",
      "166/599\n",
      "167/599\n",
      "168/599\n",
      "169/599\n",
      "170/599\n",
      "171/599\n",
      "172/599\n",
      "173/599\n",
      "174/599\n",
      "175/599\n",
      "176/599\n",
      "177/599\n",
      "178/599\n",
      "179/599\n",
      "180/599\n",
      "181/599\n",
      "182/599\n",
      "183/599\n",
      "184/599\n",
      "185/599\n",
      "186/599\n",
      "187/599\n",
      "188/599\n",
      "189/599\n",
      "190/599\n",
      "191/599\n",
      "192/599\n",
      "193/599\n",
      "194/599\n",
      "195/599\n",
      "196/599\n",
      "197/599\n",
      "198/599\n",
      "199/599\n",
      "200/599\n",
      "201/599\n",
      "202/599\n",
      "203/599\n",
      "204/599\n",
      "205/599\n",
      "206/599\n",
      "207/599\n",
      "208/599\n",
      "209/599\n",
      "210/599\n",
      "211/599\n",
      "212/599\n",
      "213/599\n",
      "214/599\n",
      "215/599\n",
      "216/599\n",
      "217/599\n",
      "218/599\n",
      "219/599\n",
      "220/599\n",
      "221/599\n",
      "222/599\n",
      "223/599\n",
      "224/599\n",
      "225/599\n",
      "226/599\n",
      "227/599\n",
      "228/599\n",
      "229/599\n",
      "230/599\n",
      "231/599\n",
      "232/599\n",
      "233/599\n",
      "234/599\n",
      "235/599\n",
      "236/599\n",
      "237/599\n",
      "238/599\n",
      "239/599\n",
      "240/599\n",
      "241/599\n",
      "242/599\n",
      "243/599\n",
      "244/599\n",
      "245/599\n",
      "246/599\n",
      "247/599\n",
      "248/599\n",
      "249/599\n",
      "250/599\n",
      "251/599\n",
      "252/599\n",
      "253/599\n",
      "254/599\n",
      "255/599\n",
      "256/599\n",
      "257/599\n",
      "258/599\n",
      "259/599\n",
      "260/599\n",
      "261/599\n",
      "262/599\n",
      "263/599\n",
      "264/599\n",
      "265/599\n",
      "266/599\n",
      "267/599\n",
      "268/599\n",
      "269/599\n",
      "270/599\n",
      "271/599\n",
      "272/599\n",
      "273/599\n",
      "274/599\n",
      "275/599\n",
      "276/599\n",
      "277/599\n",
      "278/599\n",
      "279/599\n",
      "280/599\n",
      "281/599\n",
      "282/599\n",
      "283/599\n",
      "284/599\n",
      "285/599\n",
      "286/599\n",
      "287/599\n",
      "288/599\n",
      "289/599\n",
      "290/599\n",
      "291/599\n",
      "292/599\n",
      "293/599\n",
      "294/599\n",
      "295/599\n",
      "296/599\n",
      "297/599\n",
      "298/599\n",
      "299/599\n",
      "300/599\n",
      "301/599\n",
      "302/599\n",
      "303/599\n",
      "304/599\n",
      "305/599\n",
      "306/599\n",
      "307/599\n",
      "308/599\n",
      "309/599\n",
      "310/599\n",
      "311/599\n",
      "312/599\n",
      "313/599\n",
      "314/599\n",
      "315/599\n",
      "316/599\n",
      "317/599\n",
      "318/599\n",
      "319/599\n",
      "320/599\n",
      "321/599\n",
      "322/599\n",
      "323/599\n",
      "324/599\n",
      "325/599\n",
      "326/599\n",
      "327/599\n",
      "328/599\n",
      "329/599\n",
      "330/599\n",
      "331/599\n",
      "332/599\n",
      "333/599\n",
      "334/599\n",
      "335/599\n",
      "336/599\n",
      "337/599\n",
      "338/599\n",
      "339/599\n",
      "340/599\n",
      "341/599\n",
      "342/599\n",
      "343/599\n",
      "344/599\n",
      "345/599\n",
      "346/599\n",
      "347/599\n",
      "348/599\n",
      "349/599\n",
      "350/599\n",
      "351/599\n",
      "352/599\n",
      "353/599\n",
      "354/599\n",
      "355/599\n",
      "356/599\n",
      "357/599\n",
      "358/599\n",
      "359/599\n",
      "360/599\n",
      "361/599\n",
      "362/599\n",
      "363/599\n",
      "364/599\n",
      "365/599\n",
      "366/599\n",
      "367/599\n",
      "368/599\n",
      "369/599\n",
      "370/599\n",
      "371/599\n",
      "372/599\n",
      "373/599\n",
      "374/599\n",
      "375/599\n",
      "376/599\n",
      "377/599\n",
      "378/599\n",
      "379/599\n",
      "380/599\n",
      "381/599\n",
      "382/599\n",
      "383/599\n",
      "384/599\n",
      "385/599\n",
      "386/599\n",
      "387/599\n",
      "388/599\n",
      "389/599\n",
      "390/599\n",
      "391/599\n",
      "392/599\n",
      "393/599\n",
      "394/599\n",
      "395/599\n",
      "396/599\n",
      "397/599\n",
      "398/599\n",
      "399/599\n",
      "400/599\n",
      "401/599\n",
      "402/599\n",
      "403/599\n",
      "404/599\n",
      "405/599\n",
      "406/599\n",
      "407/599\n",
      "408/599\n",
      "409/599\n",
      "410/599\n",
      "411/599\n",
      "412/599\n",
      "413/599\n",
      "414/599\n",
      "415/599\n",
      "416/599\n",
      "417/599\n",
      "418/599\n",
      "419/599\n",
      "420/599\n",
      "421/599\n",
      "422/599\n",
      "423/599\n",
      "424/599\n",
      "425/599\n",
      "426/599\n",
      "427/599\n",
      "428/599\n",
      "429/599\n",
      "430/599\n",
      "431/599\n",
      "432/599\n",
      "433/599\n",
      "434/599\n",
      "435/599\n",
      "436/599\n",
      "437/599\n",
      "438/599\n",
      "439/599\n",
      "440/599\n",
      "441/599\n",
      "442/599\n",
      "443/599\n",
      "444/599\n",
      "445/599\n",
      "446/599\n",
      "447/599\n",
      "448/599\n",
      "449/599\n",
      "450/599\n",
      "451/599\n",
      "452/599\n",
      "453/599\n",
      "454/599\n",
      "455/599\n",
      "456/599\n",
      "457/599\n",
      "458/599\n",
      "459/599\n",
      "460/599\n",
      "461/599\n",
      "462/599\n",
      "463/599\n",
      "464/599\n",
      "465/599\n",
      "466/599\n",
      "467/599\n",
      "468/599\n",
      "469/599\n",
      "470/599\n",
      "471/599\n",
      "472/599\n",
      "473/599\n",
      "474/599\n",
      "475/599\n",
      "476/599\n",
      "477/599\n",
      "478/599\n",
      "479/599\n",
      "480/599\n",
      "481/599\n",
      "482/599\n",
      "483/599\n",
      "484/599\n",
      "485/599\n",
      "486/599\n",
      "487/599\n",
      "488/599\n",
      "489/599\n",
      "490/599\n",
      "491/599\n",
      "492/599\n",
      "493/599\n",
      "494/599\n",
      "495/599\n",
      "496/599\n",
      "497/599\n",
      "498/599\n",
      "499/599\n",
      "500/599\n",
      "501/599\n",
      "502/599\n",
      "503/599\n",
      "504/599\n",
      "505/599\n",
      "506/599\n",
      "507/599\n",
      "508/599\n",
      "509/599\n",
      "510/599\n",
      "511/599\n",
      "512/599\n",
      "513/599\n",
      "514/599\n",
      "515/599\n",
      "516/599\n",
      "517/599\n",
      "518/599\n",
      "519/599\n",
      "520/599\n",
      "521/599\n",
      "522/599\n",
      "523/599\n",
      "524/599\n",
      "525/599\n",
      "526/599\n",
      "527/599\n",
      "528/599\n",
      "529/599\n",
      "530/599\n",
      "531/599\n",
      "532/599\n",
      "533/599\n",
      "534/599\n",
      "535/599\n",
      "536/599\n",
      "537/599\n",
      "538/599\n",
      "539/599\n",
      "540/599\n",
      "541/599\n",
      "542/599\n",
      "543/599\n",
      "544/599\n",
      "545/599\n",
      "546/599\n",
      "547/599\n",
      "548/599\n",
      "549/599\n",
      "550/599\n",
      "551/599\n",
      "552/599\n",
      "553/599\n",
      "554/599\n",
      "555/599\n",
      "556/599\n",
      "557/599\n",
      "558/599\n",
      "559/599\n",
      "560/599\n",
      "561/599\n",
      "562/599\n",
      "563/599\n",
      "564/599\n",
      "565/599\n",
      "566/599\n",
      "567/599\n",
      "568/599\n",
      "569/599\n",
      "570/599\n",
      "571/599\n",
      "572/599\n",
      "573/599\n",
      "574/599\n",
      "575/599\n",
      "576/599\n",
      "577/599\n",
      "578/599\n",
      "579/599\n",
      "580/599\n",
      "581/599\n",
      "582/599\n",
      "583/599\n",
      "584/599\n",
      "585/599\n",
      "586/599\n",
      "587/599\n",
      "588/599\n",
      "589/599\n",
      "590/599\n",
      "591/599\n",
      "592/599\n",
      "593/599\n",
      "594/599\n",
      "595/599\n",
      "596/599\n",
      "597/599\n",
      "598/599\n"
     ]
    }
   ],
   "source": [
    "list_articles = []\n",
    "for i,datum in enumerate(analysis_data):\n",
    "    print('{}/{}'.format(i, len(data)))\n",
    "    natural_dict = {}\n",
    "    natural_dict['article_id'] = datum['article_id']\n",
    "    natural_dict['writer_summary'] = datum['writer_summary']\n",
    "    natural_dict['text-davinci-002_summary'] = datum['text-davinci-002_summary']\n",
    "    natural_dict['overall_writer_better'] = datum['overall_writer_better']\n",
    "    natural_dict['informative_writer_better'] = datum['informative_writer_better']\n",
    "    natural_dict['writer average word lengths'] = datum['average word lengths']['writer'] \n",
    "    natural_dict['LLM average word lengths'] = datum['average word lengths']['LLM']\n",
    "    natural_dict['Writer Average Dependency tree heights'] = datum['Average Dependency tree heights']['writer']\n",
    "    natural_dict['LLM Average Dependency tree heights'] = datum['Average Dependency tree heights']['LLM']\n",
    "    natural_dict['writer_average_sentence_lengths'] = datum['average sentence lengths']['writer']\n",
    "    natural_dict['LLM_average_sentence_lengths'] = datum['average sentence lengths']['LLM']\n",
    "    natural_dict['writer_average_left_arc_length'] = datum['Arc analysis']['writer']['average left arc length']\n",
    "    natural_dict['LLM_average_left_arc_length'] = datum['Arc analysis']['LLM']['average left arc length']\n",
    "    natural_dict['writer_average_right_arc_length'] = datum['Arc analysis']['writer']['average right arc length']\n",
    "    natural_dict['LLM_average_right_arc_length'] = datum['Arc analysis']['LLM']['average right arc length']\n",
    "    natural_dict['writer_average_total_arc_length'] = datum['Arc analysis']['writer']['average total arc length']\n",
    "    natural_dict['LLM_average_total_arc_length'] = datum['Arc analysis']['LLM']['average total arc length']\n",
    "    natural_dict['writer_avg_left_subtree_height'] = datum['Left and right subtrees analysis']['writer']['avg_left_subtree_height'] \n",
    "    natural_dict['LLM_avg_left_subtree_height'] = datum['Left and right subtrees analysis']['LLM']['avg_left_subtree_height']\n",
    "    natural_dict['writer_avg_right_subtree_height'] = datum['Left and right subtrees analysis']['writer']['avg_right_subtree_height']\n",
    "    natural_dict['LLM_avg_right_subtree_height'] = datum['Left and right subtrees analysis']['LLM']['avg_right_subtree_height']\n",
    "    natural_dict['writer_num_left_subtrees'] = datum['Left and right subtrees analysis']['writer']['num_left_subtrees']\n",
    "    natural_dict['LLM_num_left_subtrees'] = datum['Left and right subtrees analysis']['LLM']['num_left_subtrees']\n",
    "    natural_dict['writer_num_right_subtrees'] = datum['Left and right subtrees analysis']['writer']['num_right_subtrees']\n",
    "    natural_dict['LLM_num_right_subtrees'] = datum['Left and right subtrees analysis']['LLM']['num_right_subtrees']\n",
    "    # natural_dict['delta_no_sentences'] = datum['Arc analysis']['writer']['number of sentences'] - datum['Arc analysis']['LLM']['number of sentences']\n",
    "    # natural_dict['no_sentences_writer'] = datum['Arc analysis']['writer']['number of sentences']\n",
    "    # natural_dict['no_sentences_llm'] = datum['Arc analysis']['LLM']['number of sentences']\n",
    "    list_articles.append(natural_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define range(range is min in both LLM and Writer summaries same for max to account for texts not in dataset) for selected features and min max scaling for selected features. These are the final values used for naturalness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dat in list_articles:\n",
    "    range_avg_dep_tree_ht = (min(df['Writer Average Dependency tree heights'].min(),df['LLM Average Dependency tree heights'].min()),max(df['Writer Average Dependency tree heights'].max(),df['LLM Average Dependency tree heights'].max()))\n",
    "    range_avg_sentence_length = (min(df['writer_average_sentence_lengths'].min(),df['LLM_average_sentence_lengths'].min()),max(df['writer_average_sentence_lengths'].max(),df['LLM_average_sentence_lengths'].max()))\n",
    "    range_avg_left_subtree_ht = (min(df['writer_avg_left_subtree_height'].min(),df['LLM_avg_left_subtree_height'].min()),max(df['writer_avg_left_subtree_height'].max(),df['LLM_avg_left_subtree_height'].max()))\n",
    "    range_avg_right_subtree_ht = (min(df['writer_avg_right_subtree_height'].min(),df['LLM_avg_right_subtree_height'].min()),max(df['writer_avg_right_subtree_height'].max(),df['LLM_avg_right_subtree_height'].max()))\n",
    "    dat['Writer Average Dependency tree heights'] = (dat['Writer Average Dependency tree heights']-range_avg_dep_tree_ht[0])/(range_avg_dep_tree_ht[1]-range_avg_dep_tree_ht[0])\n",
    "    dat['LLM Average Dependency tree heights'] = (dat['LLM Average Dependency tree heights']-range_avg_dep_tree_ht[0])/(range_avg_dep_tree_ht[1]-range_avg_dep_tree_ht[0])\n",
    "    dat['writer_average_sentence_lengths'] = (dat['writer_average_sentence_lengths']-range_avg_sentence_length[0])/(range_avg_sentence_length[1]-range_avg_sentence_length[0])\n",
    "    dat['LLM_average_sentence_lengths'] = (dat['LLM_average_sentence_lengths']-range_avg_sentence_length[0])/(range_avg_sentence_length[1]-range_avg_sentence_length[0])\n",
    "    dat['writer_avg_left_subtree_height'] = (dat['writer_avg_left_subtree_height']-range_avg_left_subtree_ht[0])/(range_avg_left_subtree_ht[1]-range_avg_left_subtree_ht[0])\n",
    "    dat['LLM_avg_left_subtree_height'] = (dat['LLM_avg_left_subtree_height']-range_avg_left_subtree_ht[0])/(range_avg_left_subtree_ht[1]-range_avg_left_subtree_ht[0])\n",
    "    dat['writer_avg_right_subtree_height'] = (dat['writer_avg_right_subtree_height']-range_avg_right_subtree_ht[0])/(range_avg_right_subtree_ht[1]-range_avg_right_subtree_ht[0])\n",
    "    dat['LLM_avg_right_subtree_height'] = (dat['LLM_avg_right_subtree_height']-range_avg_right_subtree_ht[0])/(range_avg_right_subtree_ht[1]-range_avg_right_subtree_ht[0])\n",
    "save_json(list_articles,r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/final_metrics_naturalness.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe for final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_metrics_dict = json.load(open(r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/final_metrics_naturalness.json'))\n",
    "df_features = pd.DataFrame(natural_metrics_dict)\n",
    "df_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load delta values document for feature importance measurement (not used directly for naturalness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_natural =  json.load(open(r'D:/Laptop_Projects/LLMEval-1/preprocess/experiments/data/metrics_natural.json'))\n",
    "df_delta = pd.DataFrame(data_natural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifier training to assign weights to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writer average word lengths: 0.22546514183292254\n",
      "Writer Average Dependency tree heights: 0.21132551661766943\n",
      "writer_average_sentence_lengths: 0.18634917042317664\n",
      "writer_avg_left_subtree_height: 0.1741745467970967\n",
      "writer_avg_right_subtree_height: 0.20268562432913467\n"
     ]
    }
   ],
   "source": [
    "feature_writer = [\n",
    "    'writer average word lengths',\n",
    "    # 'LLM average word lengths',\n",
    "    'Writer Average Dependency tree heights',\n",
    "    # 'LLM Average Dependency tree heights',\n",
    "    'writer_average_sentence_lengths', \n",
    "    # 'LLM_average_sentence_lengths',\n",
    "    # 'writer_average_left_arc_length', \n",
    "    # 'LLM_average_left_arc_length',\n",
    "    # 'writer_average_right_arc_length', \n",
    "    # 'LLM_average_right_arc_length',\n",
    "    # 'writer_average_total_arc_length', \n",
    "    # 'LLM_average_total_arc_length',\n",
    "    'writer_avg_left_subtree_height',\n",
    "    # 'LLM_avg_left_subtree_height',\n",
    "    'writer_avg_right_subtree_height', \n",
    "    # 'LLM_avg_right_subtree_height',\n",
    "    # 'writer_num_left_subtrees', \n",
    "    # 'LLM_num_left_subtrees',\n",
    "    # 'writer_num_right_subtrees', \n",
    "    # 'LLM_num_right_subtrees'\n",
    "]\n",
    "feature_LLM = [\n",
    "    # 'writer average word lengths',\n",
    "    'LLM average word lengths',\n",
    "    # 'Writer Average Dependency tree heights',\n",
    "    'LLM Average Dependency tree heights',\n",
    "    # 'writer_average_sentence_lengths', \n",
    "    'LLM_average_sentence_lengths',\n",
    "    # 'writer_average_left_arc_length', \n",
    "    # 'LLM_average_left_arc_length',\n",
    "    # 'writer_average_right_arc_length', \n",
    "    # 'LLM_average_right_arc_length',\n",
    "    # 'writer_average_total_arc_length', \n",
    "    # 'LLM_average_total_arc_length',\n",
    "    # 'writer_avg_left_subtree_height',\n",
    "    'LLM_avg_left_subtree_height',\n",
    "    # 'writer_avg_right_subtree_height', \n",
    "    'LLM_avg_right_subtree_height',\n",
    "    # 'writer_num_left_subtrees', \n",
    "    # 'LLM_num_left_subtrees',\n",
    "    # 'writer_num_right_subtrees', \n",
    "    # 'LLM_num_right_subtrees'\n",
    "]\n",
    "\n",
    "# encode target variable\n",
    "encoder = LabelEncoder()\n",
    "df['overall_writer_better'] = [str(value) for value in df['overall_writer_better']]\n",
    "df['informative_writer_better'] = [str(value) for value in df['informative_writer_better']]\n",
    "df['overall_writer_better'] = encoder.fit_transform(df['overall_writer_better'])\n",
    "df['informative_writer_better'] = encoder.fit_transform(df['informative_writer_better'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_writer], df['overall_writer_better'], test_size=0.2, random_state=42) # change to feature_LLM for LLM weights\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "for feature, importance in zip(feature_writer, importances):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating naturalness for all writer and LLM summaries in the dataset(Same weights are taken for both to prevent bias in one category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_writer = {\n",
    "    # 'writer average word lengths': 0.29729001098302515,\n",
    "    'Writer Average Dependency tree heights': 0.2826426317853948,\n",
    "    'writer_average_sentence_lengths': 0.2522139996530825,\n",
    "    # 'writer_average_left_arc_length': 0.09813570839334515,\n",
    "    # 'writer_average_right_arc_length': 0.10403547536278904,\n",
    "    # 'writer_average_total_arc_length': 0.10209866577981681,\n",
    "    'writer_avg_left_subtree_height': 0.23287886122532872,\n",
    "    'writer_avg_right_subtree_height': 0.2458490930520618,\n",
    "    # 'writer_num_left_subtrees': 0.2614301716393748,\n",
    "    # 'writer_num_right_subtrees': 0.23331946113659002\n",
    "}\n",
    "\n",
    "weights_llm = {\n",
    "    # 'LLM average word lengths': 0.24348774347757085,\n",
    "    'LLM Average Dependency tree heights': 0.2826426317853948,\n",
    "    'LLM_average_sentence_lengths': 0.2522139996530825,\n",
    "    # 'LLM_average_left_arc_length': 0.08065295776913092,\n",
    "    # 'LLM_average_right_arc_length': 0.08178144517768533,\n",
    "    # 'LLM_average_total_arc_length': 0.07702517940148237,\n",
    "    'LLM_avg_left_subtree_height': 0.23287886122532872,\n",
    "    'LLM_avg_right_subtree_height': 0.2458490930520618,\n",
    "    # 'LLM_num_left_subtrees': 0.2545225325226861,\n",
    "    # 'LLM_num_right_subtrees': 0.27698367937873053\n",
    "}\n",
    "\n",
    "def calculate_naturalness_score(features, weights):\n",
    "    inverses = 1.0-features\n",
    "    naturalness_score = np.mean(np.dot(inverses, weights))\n",
    "    return naturalness_score\n",
    "\n",
    "df_features['naturalness_score_llm'] = df_features.apply(lambda row: calculate_naturalness_score(row[weights_llm.keys()], np.array(list(weights_llm.values()))), axis=1)\n",
    "df_features['naturalness_score_writer'] = df_features.apply(lambda row: calculate_naturalness_score(row[weights_writer.keys()], np.array(list(weights_writer.values()))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = df_features.to_csv('df_features.csv', index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Feature  Importance\n",
      "2          delta_average_sentence_lengths    0.446753\n",
      "1   delta Average Dependency tree heights    0.377018\n",
      "6          delta_avg_right_subtree_height    0.336561\n",
      "5           delta_avg_left_subtree_height    0.314951\n",
      "4          delta_average_right_arc_length    0.161124\n",
      "11                       no_sentences_llm    0.111670\n",
      "8                delta_num_right_subtrees    0.101071\n",
      "3           delta_average_left_arc_length    0.081216\n",
      "7                 delta_num_left_subtrees    0.062296\n",
      "0              delta average word lengths   -0.100490\n",
      "9                      delta_no_sentences   -0.339387\n",
      "10                    no_sentences_writer   -0.511270\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = [\n",
    "    'delta average word lengths', \n",
    "    'delta Average Dependency tree heights',\n",
    "    'delta_average_sentence_lengths',\n",
    "    'delta_average_left_arc_length',\n",
    "    'delta_average_right_arc_length',\n",
    "    'delta_avg_left_subtree_height',\n",
    "    'delta_avg_right_subtree_height',\n",
    "    'delta_num_left_subtrees',\n",
    "    'delta_num_right_subtrees',\n",
    "    'delta_no_sentences',\n",
    "    'no_sentences_writer',\n",
    "    'no_sentences_llm'\n",
    "]\n",
    "x = df_delta.loc[:, feature_cols].values\n",
    "pca = PCA(n_components=1)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "importance = pca.components_[0]\n",
    "feature_importance = pd.DataFrame(list(zip(feature_cols, importance)), columns=['Feature', 'Importance'])\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use delta values for assigning weights(Problem: both LLM and writer summaries will have the same naturalness score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta average word lengths: 0.16009739275905313\n",
      "delta_average_sentence_lengths: 0.14983639765647247\n",
      "delta_average_left_arc_length: 0.13168315442524714\n",
      "delta_average_right_arc_length: 0.1377145357421385\n",
      "delta_num_left_subtrees: 0.22254152876206512\n",
      "delta_num_right_subtrees: 0.19812699065502357\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    'delta average word lengths', \n",
    "    'delta Average Dependency tree heights',\n",
    "    'delta_average_sentence_lengths',\n",
    "    'delta_average_left_arc_length',\n",
    "    'delta_average_right_arc_length',\n",
    "    'delta_avg_left_subtree_height',\n",
    "    'delta_avg_right_subtree_height',\n",
    "    'delta_num_left_subtrees',\n",
    "    'delta_num_right_subtrees',\n",
    "    'delta_no_sentences',\n",
    "    'no_sentences_writer',\n",
    "    'no_sentences_llm'\n",
    "]\n",
    "df['overall_writer_better'] = [str(value) for value in df['overall_writer_better']]\n",
    "df['informative_writer_better'] = [str(value) for value in df['informative_writer_better']]\n",
    "encoder = LabelEncoder()\n",
    "df['overall_writer_better'] = encoder.fit_transform(df['overall_writer_better'])\n",
    "df['informative_writer_better'] = encoder.fit_transform(df['informative_writer_better'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df['overall_writer_better'], test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "for feature, importance in zip(feature_cols, importances):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
