{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (0.1.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (0.4.1)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Collecting word-mover-distance\n",
      "  Downloading word_mover_distance-0.0.3.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from rouge-score) (1.24.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (2023.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from evaluate) (0.13.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (from gensim) (2.0.5)\n",
      "Collecting pyemd (from word-mover-distance)\n",
      "  Downloading pyemd-1.0.0-cp311-cp311-win_amd64.whl (147 kB)\n",
      "     ---------------------------------------- 0.0/148.0 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/148.0 kB ? eta -:--:--\n",
      "     ------------------------------- ------ 122.9/148.0 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 148.0/148.0 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyfume in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: simpful in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.1)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\aryam\\appdata\\roaming\\python\\python311\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Building wheels for collected packages: word-mover-distance\n",
      "  Building wheel for word-mover-distance (setup.py): started\n",
      "  Building wheel for word-mover-distance (setup.py): finished with status 'done'\n",
      "  Created wheel for word-mover-distance: filename=word_mover_distance-0.0.3-py3-none-any.whl size=4351 sha256=6b63bfd732316a0abcfb69dbf6a7621d72c31e027c9cc17234977b0d4878bf87\n",
      "  Stored in directory: c:\\users\\aryam\\appdata\\local\\pip\\cache\\wheels\\5b\\a3\\b5\\b43b98002cc219fbe9079ccd74607977b66146eb829998f6ca\n",
      "Successfully built word-mover-distance\n",
      "Installing collected packages: pyemd, word-mover-distance\n",
      "Successfully installed pyemd-1.0.0 word-mover-distance-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk rouge-score evaluate gensim word-mover-distance transformers POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load\n",
    "import evaluate\n",
    "import numpy \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import ot\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from metrics import distances,eval\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scorers:\n",
    "    \n",
    "    # BLEU\n",
    "    def compute_bleu(self,reference, candidate):\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "\n",
    "        bleu_1 = sentence_bleu([reference_tokens], candidate_tokens, weights=(1, 0, 0, 0))\n",
    "        bleu_2 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "        bleu_3 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "        bleu_4 = sentence_bleu([reference_tokens], candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        return bleu_1, bleu_2, bleu_3, bleu_4\n",
    "    \n",
    "    # ROGUE\n",
    "    def compute_rouge(self,reference, candidate):\n",
    "    # Convert reference and candidate sentences to lists of tokens\n",
    "        reference_tokens = reference.lower().split()\n",
    "        candidate_tokens = candidate.lower().split()\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        scores = scorer.score(reference, candidate)\n",
    "\n",
    "        # print(f\"ROUGE-1 Precision: {scores['rouge1'].precision:.4f}\")\n",
    "        # print(f\"ROUGE-1 Recall: {scores['rouge1'].recall:.4f}\")\n",
    "        # print(f\"ROUGE-1 F1-score: {scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "        # print(f\"ROUGE-2 Precision: {scores['rouge2'].precision:.4f}\")\n",
    "        # print(f\"ROUGE-2 Recall: {scores['rouge2'].recall:.4f}\")\n",
    "        # print(f\"ROUGE-2 F1-score: {scores['rouge2'].fmeasure:.4f}\")\n",
    "\n",
    "        # print(f\"ROUGE-L Precision: {scores['rougeL'].precision:.4f}\")\n",
    "        # print(f\"ROUGE-L Recall: {scores['rougeL'].recall:.4f}\")\n",
    "        # print(f\"ROUGE-L F1-score: {scores['rougeL'].fmeasure:.4f}\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "    # METEOR\n",
    "    def compute_meteor(self,reference, candidate):\n",
    "\n",
    "        reference =  nltk.word_tokenize(reference.lower())\n",
    "        candidate =  nltk.word_tokenize(candidate.lower())\n",
    "\n",
    "        score = meteor_score([reference], candidate)\n",
    "\n",
    "        return score\n",
    "\n",
    "    # WORD MOVERS DISTANCE    \n",
    "    def compute_wmd(self,sentence_a,sentence_b,model):\n",
    "\n",
    "        sentence_a = sentence_a.lower().split()\n",
    "        sentence_b = sentence_b.lower().split()\n",
    "\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "        sentence_a = [w for w in sentence_a if w not in stop_words]\n",
    "        sentence_b = [w for w in sentence_b if w not in stop_words]\n",
    "\n",
    "        # model = api.load('word2vec-google-news-300')\n",
    "        distance = model.wmdistance(sentence_a,sentence_b)\n",
    "        # print(distance)\n",
    "\n",
    "        return distance\n",
    "    \n",
    "    # TRANSLATION ERROR RATE\n",
    "    def compute_ter(self,reference, candidate):\n",
    "\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "\n",
    "        substitutions = nltk.edit_distance(ref_tokens, cand_tokens)\n",
    "        deletions = len(ref_tokens) - len(set(ref_tokens) & set(cand_tokens))\n",
    "        insertions = len(cand_tokens) - len(set(ref_tokens) & set(cand_tokens))\n",
    "\n",
    "        reference_length = len(ref_tokens)\n",
    "        ter = (substitutions + deletions + insertions) / reference_length\n",
    "\n",
    "        return ter\n",
    "\n",
    "    def compute_perplexity(self,text):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        inputs = tokenizer(text, return_tensors = \"pt\")\n",
    "        loss = torch.nn.CrossEntropyLoss() \n",
    "        final_loss = model(input_ids = inputs[\"input_ids\"], labels = inputs[\"input_ids\"]).loss\n",
    "        ppl = torch.exp(final_loss)\n",
    "        return ppl\n",
    "        # print(ppl)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, filepath=r'new_data.json'):\n",
    "    with open(filepath, 'w') as fp:\n",
    "        json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 Score: 0.2759\n",
      "BLEU-2 Score: 0.2601\n",
      "BLEU-3 Score: 0.2328\n",
      "BLEU-4 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"The president of the United States of America\"\n",
    "candidate_text = \"The president of India\"\n",
    "scorer = Scorers()\n",
    "bleu_1, bleu_2, bleu_3, bleu_4 = scorer.compute_bleu(reference_text, candidate_text)\n",
    "print(f\"BLEU-1 Score: {bleu_1:.4f}\")\n",
    "print(f\"BLEU-2 Score: {bleu_2:.4f}\")\n",
    "print(f\"BLEU-3 Score: {bleu_3:.4f}\")\n",
    "print(f\"BLEU-4 Score: {bleu_4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8607883257981531"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = api.load('word2vec-google-news-300')\n",
    "scorer.compute_wmd(reference_text,candidate_text,model)\n",
    "print(\"$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "scorer.compute_wmd(reference_text,candidate_text,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.1974\n"
     ]
    }
   ],
   "source": [
    "meteor = scorer.compute_meteor(reference_text, candidate_text)\n",
    "print(f\"METEOR Score: {meteor:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(r'data/pairwise_evaluation_w_embeddings.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'metrics.distances' has no attribute 'linear_regression_distances'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m writer_summary_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([datum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwriter_summary_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m datum \u001b[38;5;129;01min\u001b[39;00m data])\n\u001b[0;32m      3\u001b[0m llm_summary_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([datum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_summary_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m datum \u001b[38;5;129;01min\u001b[39;00m data])\n\u001b[1;32m----> 5\u001b[0m distances_writer \u001b[38;5;241m=\u001b[39m distances\u001b[38;5;241m.\u001b[39mlinear_regression_distances(full_embeddings,writer_summary_embeddings)\n\u001b[0;32m      6\u001b[0m distances_writer\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'metrics.distances' has no attribute 'linear_regression_distances'"
     ]
    }
   ],
   "source": [
    "full_embeddings = np.array([datum['full_embedding'] for datum in data])\n",
    "writer_summary_embeddings = np.array([datum['writer_summary_embedding'] for datum in data])\n",
    "llm_summary_embeddings = np.array([datum['llm_summary_embedding'] for datum in data])\n",
    "\n",
    "distances_writer = distances.linear_regression_distances(full_embeddings,writer_summary_embeddings)\n",
    "distances_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all Scores for all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/599\n",
      "2/599\n",
      "3/599\n",
      "4/599\n",
      "5/599\n",
      "6/599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/599\n",
      "8/599\n",
      "9/599\n",
      "10/599\n",
      "11/599\n",
      "12/599\n",
      "13/599\n",
      "14/599\n",
      "15/599\n",
      "16/599\n",
      "17/599\n",
      "18/599\n",
      "19/599\n",
      "20/599\n",
      "21/599\n",
      "22/599\n",
      "23/599\n",
      "24/599\n",
      "25/599\n",
      "26/599\n",
      "27/599\n",
      "28/599\n",
      "29/599\n",
      "30/599\n",
      "31/599\n",
      "32/599\n",
      "33/599\n",
      "34/599\n",
      "35/599\n",
      "36/599\n",
      "37/599\n",
      "38/599\n",
      "39/599\n",
      "40/599\n",
      "41/599\n",
      "42/599\n",
      "43/599\n",
      "44/599\n",
      "45/599\n",
      "46/599\n",
      "47/599\n",
      "48/599\n",
      "49/599\n",
      "50/599\n",
      "51/599\n",
      "52/599\n",
      "53/599\n",
      "54/599\n",
      "55/599\n",
      "56/599\n",
      "57/599\n",
      "58/599\n",
      "59/599\n",
      "60/599\n",
      "61/599\n",
      "62/599\n",
      "63/599\n",
      "64/599\n",
      "65/599\n",
      "66/599\n",
      "67/599\n",
      "68/599\n",
      "69/599\n",
      "70/599\n",
      "71/599\n",
      "72/599\n",
      "73/599\n",
      "74/599\n",
      "75/599\n",
      "76/599\n",
      "77/599\n",
      "78/599\n",
      "79/599\n",
      "80/599\n",
      "81/599\n",
      "82/599\n",
      "83/599\n",
      "84/599\n",
      "85/599\n",
      "86/599\n",
      "87/599\n",
      "88/599\n",
      "89/599\n",
      "90/599\n",
      "91/599\n",
      "92/599\n",
      "93/599\n",
      "94/599\n",
      "95/599\n",
      "96/599\n",
      "97/599\n",
      "98/599\n",
      "99/599\n",
      "100/599\n",
      "101/599\n",
      "102/599\n",
      "103/599\n",
      "104/599\n",
      "105/599\n",
      "106/599\n",
      "107/599\n",
      "108/599\n",
      "109/599\n",
      "110/599\n",
      "111/599\n",
      "112/599\n",
      "113/599\n",
      "114/599\n",
      "115/599\n",
      "116/599\n",
      "117/599\n",
      "118/599\n",
      "119/599\n",
      "120/599\n",
      "121/599\n",
      "122/599\n",
      "123/599\n",
      "124/599\n",
      "125/599\n",
      "126/599\n",
      "127/599\n",
      "128/599\n",
      "129/599\n",
      "130/599\n",
      "131/599\n",
      "132/599\n",
      "133/599\n",
      "134/599\n",
      "135/599\n",
      "136/599\n",
      "137/599\n",
      "138/599\n",
      "139/599\n",
      "140/599\n",
      "141/599\n",
      "142/599\n",
      "143/599\n",
      "144/599\n",
      "145/599\n",
      "146/599\n",
      "147/599\n",
      "148/599\n",
      "149/599\n",
      "150/599\n",
      "151/599\n",
      "152/599\n",
      "153/599\n",
      "154/599\n",
      "155/599\n",
      "156/599\n",
      "157/599\n",
      "158/599\n",
      "159/599\n",
      "160/599\n",
      "161/599\n",
      "162/599\n",
      "163/599\n",
      "164/599\n",
      "165/599\n",
      "166/599\n",
      "167/599\n",
      "168/599\n",
      "169/599\n",
      "170/599\n",
      "171/599\n",
      "172/599\n",
      "173/599\n",
      "174/599\n",
      "175/599\n",
      "176/599\n",
      "177/599\n",
      "178/599\n",
      "179/599\n",
      "180/599\n",
      "181/599\n",
      "182/599\n",
      "183/599\n",
      "184/599\n",
      "185/599\n",
      "186/599\n",
      "187/599\n",
      "188/599\n",
      "189/599\n",
      "190/599\n",
      "191/599\n",
      "192/599\n",
      "193/599\n",
      "194/599\n",
      "195/599\n",
      "196/599\n",
      "197/599\n",
      "198/599\n",
      "199/599\n",
      "200/599\n",
      "201/599\n",
      "202/599\n",
      "203/599\n",
      "204/599\n",
      "205/599\n",
      "206/599\n",
      "207/599\n",
      "208/599\n",
      "209/599\n",
      "210/599\n",
      "211/599\n",
      "212/599\n",
      "213/599\n",
      "214/599\n",
      "215/599\n",
      "216/599\n",
      "217/599\n",
      "218/599\n",
      "219/599\n",
      "220/599\n",
      "221/599\n",
      "222/599\n",
      "223/599\n",
      "224/599\n",
      "225/599\n",
      "226/599\n",
      "227/599\n",
      "228/599\n",
      "229/599\n",
      "230/599\n",
      "231/599\n",
      "232/599\n",
      "233/599\n",
      "234/599\n",
      "235/599\n",
      "236/599\n",
      "237/599\n",
      "238/599\n",
      "239/599\n",
      "240/599\n",
      "241/599\n",
      "242/599\n",
      "243/599\n",
      "244/599\n",
      "245/599\n",
      "246/599\n",
      "247/599\n",
      "248/599\n",
      "249/599\n",
      "250/599\n",
      "251/599\n",
      "252/599\n",
      "253/599\n",
      "254/599\n",
      "255/599\n",
      "256/599\n",
      "257/599\n",
      "258/599\n",
      "259/599\n",
      "260/599\n",
      "261/599\n",
      "262/599\n",
      "263/599\n",
      "264/599\n",
      "265/599\n",
      "266/599\n",
      "267/599\n",
      "268/599\n",
      "269/599\n",
      "270/599\n",
      "271/599\n",
      "272/599\n",
      "273/599\n",
      "274/599\n",
      "275/599\n",
      "276/599\n",
      "277/599\n",
      "278/599\n",
      "279/599\n",
      "280/599\n",
      "281/599\n",
      "282/599\n",
      "283/599\n",
      "284/599\n",
      "285/599\n",
      "286/599\n",
      "287/599\n",
      "288/599\n",
      "289/599\n",
      "290/599\n",
      "291/599\n",
      "292/599\n",
      "293/599\n",
      "294/599\n",
      "295/599\n",
      "296/599\n",
      "297/599\n",
      "298/599\n",
      "299/599\n",
      "300/599\n",
      "301/599\n",
      "302/599\n",
      "303/599\n",
      "304/599\n",
      "305/599\n",
      "306/599\n",
      "307/599\n",
      "308/599\n",
      "309/599\n",
      "310/599\n",
      "311/599\n",
      "312/599\n",
      "313/599\n",
      "314/599\n",
      "315/599\n",
      "316/599\n",
      "317/599\n",
      "318/599\n",
      "319/599\n",
      "320/599\n",
      "321/599\n",
      "322/599\n",
      "323/599\n",
      "324/599\n",
      "325/599\n",
      "326/599\n",
      "327/599\n",
      "328/599\n",
      "329/599\n",
      "330/599\n",
      "331/599\n",
      "332/599\n",
      "333/599\n",
      "334/599\n",
      "335/599\n",
      "336/599\n",
      "337/599\n",
      "338/599\n",
      "339/599\n",
      "340/599\n",
      "341/599\n",
      "342/599\n",
      "343/599\n",
      "344/599\n",
      "345/599\n",
      "346/599\n",
      "347/599\n",
      "348/599\n",
      "349/599\n",
      "350/599\n",
      "351/599\n",
      "352/599\n",
      "353/599\n",
      "354/599\n",
      "355/599\n",
      "356/599\n",
      "357/599\n",
      "358/599\n",
      "359/599\n",
      "360/599\n",
      "361/599\n",
      "362/599\n",
      "363/599\n",
      "364/599\n",
      "365/599\n",
      "366/599\n",
      "367/599\n",
      "368/599\n",
      "369/599\n",
      "370/599\n",
      "371/599\n",
      "372/599\n",
      "373/599\n",
      "374/599\n",
      "375/599\n",
      "376/599\n",
      "377/599\n",
      "378/599\n",
      "379/599\n",
      "380/599\n",
      "381/599\n",
      "382/599\n",
      "383/599\n",
      "384/599\n",
      "385/599\n",
      "386/599\n",
      "387/599\n",
      "388/599\n",
      "389/599\n",
      "390/599\n",
      "391/599\n",
      "392/599\n",
      "393/599\n",
      "394/599\n",
      "395/599\n",
      "396/599\n",
      "397/599\n",
      "398/599\n",
      "399/599\n",
      "400/599\n",
      "401/599\n",
      "402/599\n",
      "403/599\n",
      "404/599\n",
      "405/599\n",
      "406/599\n",
      "407/599\n",
      "408/599\n",
      "409/599\n",
      "410/599\n",
      "411/599\n",
      "412/599\n",
      "413/599\n",
      "414/599\n",
      "415/599\n",
      "416/599\n",
      "417/599\n",
      "418/599\n",
      "419/599\n",
      "420/599\n",
      "421/599\n",
      "422/599\n",
      "423/599\n",
      "424/599\n",
      "425/599\n",
      "426/599\n",
      "427/599\n",
      "428/599\n",
      "429/599\n",
      "430/599\n",
      "431/599\n",
      "432/599\n",
      "433/599\n",
      "434/599\n",
      "435/599\n",
      "436/599\n",
      "437/599\n",
      "438/599\n",
      "439/599\n",
      "440/599\n",
      "441/599\n",
      "442/599\n",
      "443/599\n",
      "444/599\n",
      "445/599\n",
      "446/599\n",
      "447/599\n",
      "448/599\n",
      "449/599\n",
      "450/599\n",
      "451/599\n",
      "452/599\n",
      "453/599\n",
      "454/599\n",
      "455/599\n",
      "456/599\n",
      "457/599\n",
      "458/599\n",
      "459/599\n",
      "460/599\n",
      "461/599\n",
      "462/599\n",
      "463/599\n",
      "464/599\n",
      "465/599\n",
      "466/599\n",
      "467/599\n",
      "468/599\n",
      "469/599\n",
      "470/599\n",
      "471/599\n",
      "472/599\n",
      "473/599\n",
      "474/599\n",
      "475/599\n",
      "476/599\n",
      "477/599\n",
      "478/599\n",
      "479/599\n",
      "480/599\n",
      "481/599\n",
      "482/599\n",
      "483/599\n",
      "484/599\n",
      "485/599\n",
      "486/599\n",
      "487/599\n",
      "488/599\n",
      "489/599\n",
      "490/599\n",
      "491/599\n",
      "492/599\n",
      "493/599\n",
      "494/599\n",
      "495/599\n",
      "496/599\n",
      "497/599\n",
      "498/599\n",
      "499/599\n",
      "500/599\n",
      "501/599\n",
      "502/599\n",
      "503/599\n",
      "504/599\n",
      "505/599\n",
      "506/599\n",
      "507/599\n",
      "508/599\n",
      "509/599\n",
      "510/599\n",
      "511/599\n",
      "512/599\n",
      "513/599\n",
      "514/599\n",
      "515/599\n",
      "516/599\n",
      "517/599\n",
      "518/599\n",
      "519/599\n",
      "520/599\n",
      "521/599\n",
      "522/599\n",
      "523/599\n",
      "524/599\n",
      "525/599\n",
      "526/599\n",
      "527/599\n",
      "528/599\n",
      "529/599\n",
      "530/599\n",
      "531/599\n",
      "532/599\n",
      "533/599\n",
      "534/599\n",
      "535/599\n",
      "536/599\n",
      "537/599\n",
      "538/599\n",
      "539/599\n",
      "540/599\n",
      "541/599\n",
      "542/599\n",
      "543/599\n",
      "544/599\n",
      "545/599\n",
      "546/599\n",
      "547/599\n",
      "548/599\n",
      "549/599\n",
      "550/599\n",
      "551/599\n",
      "552/599\n",
      "553/599\n",
      "554/599\n",
      "555/599\n",
      "556/599\n",
      "557/599\n",
      "558/599\n",
      "559/599\n",
      "560/599\n",
      "561/599\n",
      "562/599\n",
      "563/599\n",
      "564/599\n",
      "565/599\n",
      "566/599\n",
      "567/599\n",
      "568/599\n",
      "569/599\n",
      "570/599\n",
      "571/599\n",
      "572/599\n",
      "573/599\n",
      "574/599\n",
      "575/599\n",
      "576/599\n",
      "577/599\n",
      "578/599\n",
      "579/599\n",
      "580/599\n",
      "581/599\n",
      "582/599\n",
      "583/599\n",
      "584/599\n",
      "585/599\n",
      "586/599\n",
      "587/599\n",
      "588/599\n",
      "589/599\n",
      "590/599\n",
      "591/599\n",
      "592/599\n",
      "593/599\n",
      "594/599\n",
      "595/599\n",
      "596/599\n",
      "597/599\n",
      "598/599\n"
     ]
    }
   ],
   "source": [
    "scorer = Scorers()\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "full_embeddings = np.array([datum['full_embedding'] for datum in data])\n",
    "writer_summary_embeddings = np.array([datum['writer_summary_embedding'] for datum in data])\n",
    "llm_summary_embeddings = np.array([datum['llm_summary_embedding'] for datum in data])\n",
    "writer_distances = distances.linear_regression_distance(full_embeddings, writer_summary_embeddings)\n",
    "llm_distances = distances.linear_regression_distance(full_embeddings, llm_summary_embeddings)\n",
    "\n",
    "for i,datum in enumerate(data):\n",
    "\n",
    "    print('{}/{}'.format(i, len(data)))\n",
    "    writer_summ = datum['writer_summary']\n",
    "    llm_summ = datum['text-davinci-002_summary']\n",
    "    article = datum['article_text']\n",
    "\n",
    "    bleu_1,bleu_2,bleu_3,bleu_4 = scorer.compute_bleu(article,writer_summ)\n",
    "    datum['bleu_1_writer'] = bleu_1\n",
    "    datum['bleu_2_writer'] = bleu_2\n",
    "    datum['bleu_3_writer'] = bleu_3\n",
    "    datum['bleu_4_writer'] = bleu_4\n",
    "\n",
    "    bleu_1,bleu_2,bleu_3,bleu_4 = scorer.compute_bleu(article,llm_summ)\n",
    "    datum['bleu_1_llm'] = bleu_1\n",
    "    datum['bleu_2_llm'] = bleu_2\n",
    "    datum['bleu_3_llm'] = bleu_3\n",
    "    datum['bleu_4_llm'] = bleu_4\n",
    "\n",
    "    rogue = scorer.compute_rouge(article,writer_summ)\n",
    "    datum['rogue_1_writer'] = rogue['rouge1'].fmeasure\n",
    "    datum['rogue_2_writer'] = rogue['rouge2'].fmeasure\n",
    "    datum['rogue_L_writer'] = rogue['rougeL'].fmeasure\n",
    "\n",
    "    rogue = scorer.compute_rouge(article,llm_summ)\n",
    "    datum['rogue_1_llm'] = rogue['rouge1'].fmeasure\n",
    "    datum['rogue_2_llm'] = rogue['rouge2'].fmeasure\n",
    "    datum['rogue_L_llm'] = rogue['rougeL'].fmeasure\n",
    "\n",
    "    meteor = scorer.compute_meteor(article,writer_summ)\n",
    "    datum['meteor_writer'] = meteor\n",
    "\n",
    "    meteor = scorer.compute_meteor(article,llm_summ)\n",
    "    datum['meteor_llm'] = meteor\n",
    "\n",
    "    wmd = scorer.compute_wmd(article,writer_summ,model)\n",
    "    datum['WMD_writer'] = wmd\n",
    "\n",
    "    wmd = scorer.compute_wmd(article,llm_summ,model)\n",
    "    datum['WMD_llm'] = wmd\n",
    "\n",
    "    ter = scorer.compute_ter(article,writer_summ)\n",
    "    datum['TER_writer'] = ter\n",
    "\n",
    "    ter = scorer.compute_ter(article,llm_summ)\n",
    "    datum['TER_llm'] = ter\n",
    "\n",
    "    cosine_dist = distances.cosine_distance(datum['full_embedding'],datum['writer_summary_embedding'])\n",
    "    datum['cosine_writer'] = cosine_dist\n",
    "\n",
    "    cosine_dist = distances.cosine_distance(datum['full_embedding'],datum['llm_summary_embedding'])\n",
    "    datum['cosine_llm'] = cosine_dist\n",
    "\n",
    "    datum['lr_dist_writer'] = writer_distances[i]\n",
    "    datum['lr_dist_llm'] = llm_distances[i]\n",
    "\n",
    "    perplexity_writer = scorer.compute_perplexity(writer_summ)\n",
    "    perplexity_llm = scorer.compute_perplexity(llm_summ)\n",
    "    perp_writer = perplexity_writer.item()\n",
    "    perp_llm = perplexity_llm.item()\n",
    "    datum['Writer_Perplexity'] = perp_writer\n",
    "    datum['LLM_Perplexity'] = perp_llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(data, r'data/final_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = json.load(open(r'data/final_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>writer_id</th>\n",
       "      <th>evaluator_id</th>\n",
       "      <th>article_text</th>\n",
       "      <th>writer_summary</th>\n",
       "      <th>text-davinci-002_summary</th>\n",
       "      <th>overall_writer_better</th>\n",
       "      <th>informative_writer_better</th>\n",
       "      <th>full_embedding</th>\n",
       "      <th>writer_summary_embedding</th>\n",
       "      <th>...</th>\n",
       "      <th>meteor_writer</th>\n",
       "      <th>meteor_llm</th>\n",
       "      <th>WMD_writer</th>\n",
       "      <th>WMD_llm</th>\n",
       "      <th>TER_writer</th>\n",
       "      <th>TER_llm</th>\n",
       "      <th>cosine_writer</th>\n",
       "      <th>cosine_llm</th>\n",
       "      <th>Writer_Perplexity</th>\n",
       "      <th>LLM_Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18cba9a8f2f64055a707452638182303</td>\n",
       "      <td>133d66ad12ab449e8c607d188b65e948</td>\n",
       "      <td>9d49ddd0-7c67-4394-8d6b-e685a982e956</td>\n",
       "      <td>Baltimore's mayor has sacked the US city's pol...</td>\n",
       "      <td>The mayor of Baltimore fired the police chief ...</td>\n",
       "      <td>The mayor of Baltimore has sacked the city's p...</td>\n",
       "      <td>False</td>\n",
       "      <td>Equally Good</td>\n",
       "      <td>[-0.007163366, 0.0075295228, -0.022528652, -0....</td>\n",
       "      <td>[-0.0027573644, -0.0059608207, -0.017378185, -...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073195</td>\n",
       "      <td>0.083950</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>0.895712</td>\n",
       "      <td>1.937349</td>\n",
       "      <td>1.816867</td>\n",
       "      <td>0.904066</td>\n",
       "      <td>0.968363</td>\n",
       "      <td>22.838551</td>\n",
       "      <td>17.785681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66f39853ad2b437c8bdca86ae74bb35f</td>\n",
       "      <td>85b4d7406d144eacaede6397fafe06b9</td>\n",
       "      <td>0ec347ce-79c1-4495-8f84-43f2f57deb82</td>\n",
       "      <td>Western Sahara has welcomed Morocco's readmiss...</td>\n",
       "      <td>Morocco joined the African Union after a refer...</td>\n",
       "      <td>The article discusses Western Sahara's reactio...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0064340984, -0.013940546, 0.022131747, -0.0...</td>\n",
       "      <td>[-0.0016521142, -0.024277786, 0.004347165, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032232</td>\n",
       "      <td>0.087081</td>\n",
       "      <td>1.093312</td>\n",
       "      <td>0.970475</td>\n",
       "      <td>1.944672</td>\n",
       "      <td>1.905738</td>\n",
       "      <td>0.915527</td>\n",
       "      <td>0.949937</td>\n",
       "      <td>77.512268</td>\n",
       "      <td>22.789694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302c800172da420f9e2e80474a9cf5ec</td>\n",
       "      <td>85b4d7406d144eacaede6397fafe06b9</td>\n",
       "      <td>0ec347ce-79c1-4495-8f84-43f2f57deb82</td>\n",
       "      <td>With the new Avengers: Age of Ultron movie rel...</td>\n",
       "      <td>James Haskell is a rugby player for the London...</td>\n",
       "      <td>James Haskell, a rugby player for London Wasps...</td>\n",
       "      <td>Equally Good</td>\n",
       "      <td>Equally Good</td>\n",
       "      <td>[-0.0070248763, -0.00925884, 0.0027083454, -0....</td>\n",
       "      <td>[-0.024424886, -0.0077787954, -0.0036272286, -...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092289</td>\n",
       "      <td>0.097873</td>\n",
       "      <td>1.015305</td>\n",
       "      <td>0.916830</td>\n",
       "      <td>1.888252</td>\n",
       "      <td>1.853868</td>\n",
       "      <td>0.946460</td>\n",
       "      <td>0.945514</td>\n",
       "      <td>25.392700</td>\n",
       "      <td>19.203604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14f71296e6404651bfdcfd300ddebcf8</td>\n",
       "      <td>7c02dffbfb0348f68758c00334878ef7</td>\n",
       "      <td>d3727ca5-7197-4a03-81a0-2137ebcd52f4</td>\n",
       "      <td>UK manufacturing activity contracted in April ...</td>\n",
       "      <td>Concerns over UK manufacturing activity have b...</td>\n",
       "      <td>The Markit/CIPS manufacturing Purchasing Manag...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.023462681, -0.024742227, -0.018057255, -0....</td>\n",
       "      <td>[-0.011470091, -0.036104277, -0.009304092, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066601</td>\n",
       "      <td>0.100952</td>\n",
       "      <td>0.913022</td>\n",
       "      <td>0.855796</td>\n",
       "      <td>1.903030</td>\n",
       "      <td>1.846465</td>\n",
       "      <td>0.931565</td>\n",
       "      <td>0.969474</td>\n",
       "      <td>38.945465</td>\n",
       "      <td>27.502934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a5d2bbfb7a74067abfb31a5f4888c71</td>\n",
       "      <td>564736de98b54961a003a097c04d7b50</td>\n",
       "      <td>d3727ca5-7197-4a03-81a0-2137ebcd52f4</td>\n",
       "      <td>An obese mother who enjoyed takeaways and booz...</td>\n",
       "      <td>Lizzi, an obese mother of six, wouldn't have r...</td>\n",
       "      <td>Lizzi Crawford, 32, from Stoke-on-Trent, lost ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0010762861, 0.00084113114, 0.0077814907, -0...</td>\n",
       "      <td>[0.00010524096, 0.007291257, 0.010436634, -0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041791</td>\n",
       "      <td>0.041858</td>\n",
       "      <td>1.084467</td>\n",
       "      <td>1.077862</td>\n",
       "      <td>1.943495</td>\n",
       "      <td>1.901445</td>\n",
       "      <td>0.931857</td>\n",
       "      <td>0.952198</td>\n",
       "      <td>36.748543</td>\n",
       "      <td>43.636581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>3b82559a2edb48c7bffca655c43fe34d</td>\n",
       "      <td>b33c38a1cc7a45358cbcd30311e78ae2</td>\n",
       "      <td>d3727ca5-7197-4a03-81a0-2137ebcd52f4</td>\n",
       "      <td>It's every interviewer's worst nightmare. Ask ...</td>\n",
       "      <td>When a News 4 presenter attempted to ask Rober...</td>\n",
       "      <td>The article discusses celebrities who have wal...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.0048403596, -0.01597452, 0.029282175, -0.0...</td>\n",
       "      <td>[-0.012659721, -0.015550625, 0.025793782, -0.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>1.050061</td>\n",
       "      <td>1.100482</td>\n",
       "      <td>1.975750</td>\n",
       "      <td>1.978302</td>\n",
       "      <td>0.907267</td>\n",
       "      <td>0.893525</td>\n",
       "      <td>28.995054</td>\n",
       "      <td>22.161198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>24df2a97c6a94a40b5f291ad5e5314b0</td>\n",
       "      <td>f7427d27b63541b8b3b1099c5f32f7de</td>\n",
       "      <td>9d49ddd0-7c67-4394-8d6b-e685a982e956</td>\n",
       "      <td>The Liberal Democrats have admitted they are o...</td>\n",
       "      <td>The Liberal Democrats are optimistic about win...</td>\n",
       "      <td>The Liberal Democrats are on course to lose at...</td>\n",
       "      <td>Equally Good</td>\n",
       "      <td>Equally Good</td>\n",
       "      <td>[-0.00969115, 0.0142671615, 0.017133743, -0.01...</td>\n",
       "      <td>[-0.02096688, -0.00027541863, 0.0039957105, -0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020894</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>1.145921</td>\n",
       "      <td>1.064026</td>\n",
       "      <td>1.978064</td>\n",
       "      <td>1.956884</td>\n",
       "      <td>0.891258</td>\n",
       "      <td>0.923691</td>\n",
       "      <td>62.143101</td>\n",
       "      <td>18.201765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2c80f9196b654048b01397ebd52d3518</td>\n",
       "      <td>564736de98b54961a003a097c04d7b50</td>\n",
       "      <td>d3727ca5-7197-4a03-81a0-2137ebcd52f4</td>\n",
       "      <td>A Hertfordshire council is buying in water fro...</td>\n",
       "      <td>The Coronation Fountain in Welwyn Garden City ...</td>\n",
       "      <td>The Coronation Fountain in Welwyn Garden City,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.016982611, -0.0031427317, -0.015396845, -0....</td>\n",
       "      <td>[0.023978898, -0.008586212, -0.002757913, 0.00...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122208</td>\n",
       "      <td>0.104249</td>\n",
       "      <td>0.849500</td>\n",
       "      <td>0.966516</td>\n",
       "      <td>1.849850</td>\n",
       "      <td>1.816817</td>\n",
       "      <td>0.938393</td>\n",
       "      <td>0.953871</td>\n",
       "      <td>72.623482</td>\n",
       "      <td>59.409863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>f1d84317501a4ba285f0e81384471f6e</td>\n",
       "      <td>b33c38a1cc7a45358cbcd30311e78ae2</td>\n",
       "      <td>b6d4bf14-3323-43ad-a311-e33bb3d5fd49</td>\n",
       "      <td>Sport funding in Scotland is facing a 20% redu...</td>\n",
       "      <td>The country of Scotland is potentially facing ...</td>\n",
       "      <td>The article discusses how sport funding in Sco...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.009644198, -0.0059027453, 0.0016152562, -0...</td>\n",
       "      <td>[-0.005729174, -0.019543953, 0.0024205518, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062730</td>\n",
       "      <td>0.039922</td>\n",
       "      <td>1.011900</td>\n",
       "      <td>1.070063</td>\n",
       "      <td>1.918367</td>\n",
       "      <td>1.929705</td>\n",
       "      <td>0.936066</td>\n",
       "      <td>0.932431</td>\n",
       "      <td>40.064541</td>\n",
       "      <td>28.392483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0f1d41fcf8934fdf8fc993851ba9c6c4</td>\n",
       "      <td>f7427d27b63541b8b3b1099c5f32f7de</td>\n",
       "      <td>d3727ca5-7197-4a03-81a0-2137ebcd52f4</td>\n",
       "      <td>Leeds striker Steve Morison admits he has neve...</td>\n",
       "      <td>Leeds United has had a chaotic season, culmina...</td>\n",
       "      <td>The article discusses the recent incident wher...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.0029643083, -0.016763905, 0.030465301, -0....</td>\n",
       "      <td>[-0.009065434, -0.0055971453, 0.023255654, -0....</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.029076</td>\n",
       "      <td>1.055474</td>\n",
       "      <td>1.037585</td>\n",
       "      <td>1.957883</td>\n",
       "      <td>1.934125</td>\n",
       "      <td>0.881845</td>\n",
       "      <td>0.890931</td>\n",
       "      <td>35.723747</td>\n",
       "      <td>44.607162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           article_id                         writer_id  \\\n",
       "0    18cba9a8f2f64055a707452638182303  133d66ad12ab449e8c607d188b65e948   \n",
       "1    66f39853ad2b437c8bdca86ae74bb35f  85b4d7406d144eacaede6397fafe06b9   \n",
       "2    302c800172da420f9e2e80474a9cf5ec  85b4d7406d144eacaede6397fafe06b9   \n",
       "3    14f71296e6404651bfdcfd300ddebcf8  7c02dffbfb0348f68758c00334878ef7   \n",
       "4    5a5d2bbfb7a74067abfb31a5f4888c71  564736de98b54961a003a097c04d7b50   \n",
       "..                                ...                               ...   \n",
       "594  3b82559a2edb48c7bffca655c43fe34d  b33c38a1cc7a45358cbcd30311e78ae2   \n",
       "595  24df2a97c6a94a40b5f291ad5e5314b0  f7427d27b63541b8b3b1099c5f32f7de   \n",
       "596  2c80f9196b654048b01397ebd52d3518  564736de98b54961a003a097c04d7b50   \n",
       "597  f1d84317501a4ba285f0e81384471f6e  b33c38a1cc7a45358cbcd30311e78ae2   \n",
       "598  0f1d41fcf8934fdf8fc993851ba9c6c4  f7427d27b63541b8b3b1099c5f32f7de   \n",
       "\n",
       "                             evaluator_id  \\\n",
       "0    9d49ddd0-7c67-4394-8d6b-e685a982e956   \n",
       "1    0ec347ce-79c1-4495-8f84-43f2f57deb82   \n",
       "2    0ec347ce-79c1-4495-8f84-43f2f57deb82   \n",
       "3    d3727ca5-7197-4a03-81a0-2137ebcd52f4   \n",
       "4    d3727ca5-7197-4a03-81a0-2137ebcd52f4   \n",
       "..                                    ...   \n",
       "594  d3727ca5-7197-4a03-81a0-2137ebcd52f4   \n",
       "595  9d49ddd0-7c67-4394-8d6b-e685a982e956   \n",
       "596  d3727ca5-7197-4a03-81a0-2137ebcd52f4   \n",
       "597  b6d4bf14-3323-43ad-a311-e33bb3d5fd49   \n",
       "598  d3727ca5-7197-4a03-81a0-2137ebcd52f4   \n",
       "\n",
       "                                          article_text  \\\n",
       "0    Baltimore's mayor has sacked the US city's pol...   \n",
       "1    Western Sahara has welcomed Morocco's readmiss...   \n",
       "2    With the new Avengers: Age of Ultron movie rel...   \n",
       "3    UK manufacturing activity contracted in April ...   \n",
       "4    An obese mother who enjoyed takeaways and booz...   \n",
       "..                                                 ...   \n",
       "594  It's every interviewer's worst nightmare. Ask ...   \n",
       "595  The Liberal Democrats have admitted they are o...   \n",
       "596  A Hertfordshire council is buying in water fro...   \n",
       "597  Sport funding in Scotland is facing a 20% redu...   \n",
       "598  Leeds striker Steve Morison admits he has neve...   \n",
       "\n",
       "                                        writer_summary  \\\n",
       "0    The mayor of Baltimore fired the police chief ...   \n",
       "1    Morocco joined the African Union after a refer...   \n",
       "2    James Haskell is a rugby player for the London...   \n",
       "3    Concerns over UK manufacturing activity have b...   \n",
       "4    Lizzi, an obese mother of six, wouldn't have r...   \n",
       "..                                                 ...   \n",
       "594  When a News 4 presenter attempted to ask Rober...   \n",
       "595  The Liberal Democrats are optimistic about win...   \n",
       "596  The Coronation Fountain in Welwyn Garden City ...   \n",
       "597  The country of Scotland is potentially facing ...   \n",
       "598  Leeds United has had a chaotic season, culmina...   \n",
       "\n",
       "                              text-davinci-002_summary overall_writer_better  \\\n",
       "0    The mayor of Baltimore has sacked the city's p...                 False   \n",
       "1    The article discusses Western Sahara's reactio...                 False   \n",
       "2    James Haskell, a rugby player for London Wasps...          Equally Good   \n",
       "3    The Markit/CIPS manufacturing Purchasing Manag...                 False   \n",
       "4    Lizzi Crawford, 32, from Stoke-on-Trent, lost ...                 False   \n",
       "..                                                 ...                   ...   \n",
       "594  The article discusses celebrities who have wal...                 False   \n",
       "595  The Liberal Democrats are on course to lose at...          Equally Good   \n",
       "596  The Coronation Fountain in Welwyn Garden City,...                  True   \n",
       "597  The article discusses how sport funding in Sco...                  True   \n",
       "598  The article discusses the recent incident wher...                  True   \n",
       "\n",
       "    informative_writer_better  \\\n",
       "0                Equally Good   \n",
       "1                       False   \n",
       "2                Equally Good   \n",
       "3                       False   \n",
       "4                       False   \n",
       "..                        ...   \n",
       "594                     False   \n",
       "595              Equally Good   \n",
       "596                      True   \n",
       "597                      True   \n",
       "598                      True   \n",
       "\n",
       "                                        full_embedding  \\\n",
       "0    [-0.007163366, 0.0075295228, -0.022528652, -0....   \n",
       "1    [0.0064340984, -0.013940546, 0.022131747, -0.0...   \n",
       "2    [-0.0070248763, -0.00925884, 0.0027083454, -0....   \n",
       "3    [-0.023462681, -0.024742227, -0.018057255, -0....   \n",
       "4    [0.0010762861, 0.00084113114, 0.0077814907, -0...   \n",
       "..                                                 ...   \n",
       "594  [-0.0048403596, -0.01597452, 0.029282175, -0.0...   \n",
       "595  [-0.00969115, 0.0142671615, 0.017133743, -0.01...   \n",
       "596  [0.016982611, -0.0031427317, -0.015396845, -0....   \n",
       "597  [-0.009644198, -0.0059027453, 0.0016152562, -0...   \n",
       "598  [-0.0029643083, -0.016763905, 0.030465301, -0....   \n",
       "\n",
       "                              writer_summary_embedding  ... meteor_writer  \\\n",
       "0    [-0.0027573644, -0.0059608207, -0.017378185, -...  ...      0.073195   \n",
       "1    [-0.0016521142, -0.024277786, 0.004347165, -0....  ...      0.032232   \n",
       "2    [-0.024424886, -0.0077787954, -0.0036272286, -...  ...      0.092289   \n",
       "3    [-0.011470091, -0.036104277, -0.009304092, -0....  ...      0.066601   \n",
       "4    [0.00010524096, 0.007291257, 0.010436634, -0.0...  ...      0.041791   \n",
       "..                                                 ...  ...           ...   \n",
       "594  [-0.012659721, -0.015550625, 0.025793782, -0.0...  ...      0.019753   \n",
       "595  [-0.02096688, -0.00027541863, 0.0039957105, -0...  ...      0.020894   \n",
       "596  [0.023978898, -0.008586212, -0.002757913, 0.00...  ...      0.122208   \n",
       "597  [-0.005729174, -0.019543953, 0.0024205518, -0....  ...      0.062730   \n",
       "598  [-0.009065434, -0.0055971453, 0.023255654, -0....  ...      0.032027   \n",
       "\n",
       "     meteor_llm  WMD_writer   WMD_llm  TER_writer   TER_llm  cosine_writer  \\\n",
       "0      0.083950    0.998596  0.895712    1.937349  1.816867       0.904066   \n",
       "1      0.087081    1.093312  0.970475    1.944672  1.905738       0.915527   \n",
       "2      0.097873    1.015305  0.916830    1.888252  1.853868       0.946460   \n",
       "3      0.100952    0.913022  0.855796    1.903030  1.846465       0.931565   \n",
       "4      0.041858    1.084467  1.077862    1.943495  1.901445       0.931857   \n",
       "..          ...         ...       ...         ...       ...            ...   \n",
       "594    0.011638    1.050061  1.100482    1.975750  1.978302       0.907267   \n",
       "595    0.023619    1.145921  1.064026    1.978064  1.956884       0.891258   \n",
       "596    0.104249    0.849500  0.966516    1.849850  1.816817       0.938393   \n",
       "597    0.039922    1.011900  1.070063    1.918367  1.929705       0.936066   \n",
       "598    0.029076    1.055474  1.037585    1.957883  1.934125       0.881845   \n",
       "\n",
       "     cosine_llm  Writer_Perplexity  LLM_Perplexity  \n",
       "0      0.968363          22.838551       17.785681  \n",
       "1      0.949937          77.512268       22.789694  \n",
       "2      0.945514          25.392700       19.203604  \n",
       "3      0.969474          38.945465       27.502934  \n",
       "4      0.952198          36.748543       43.636581  \n",
       "..          ...                ...             ...  \n",
       "594    0.893525          28.995054       22.161198  \n",
       "595    0.923691          62.143101       18.201765  \n",
       "596    0.953871          72.623482       59.409863  \n",
       "597    0.932431          40.064541       28.392483  \n",
       "598    0.890931          35.723747       44.607162  \n",
       "\n",
       "[599 rows x 37 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = df.drop(columns=['writer_id', 'evaluator_id','article_text','writer_summary','text-davinci-002_summary','article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['overall_writer_better'] = [str(value) for value in final_data['overall_writer_better']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['informative_writer_better'] = [str(value) for value in final_data['informative_writer_better']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "final_data['overall_writer_better'] = encoder.fit_transform(final_data['overall_writer_better'])\n",
    "final_data['informative_writer_better'] = encoder.fit_transform(final_data['informative_writer_better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data.drop(columns=['overall_writer_better','full_embedding','writer_summary_embedding','llm_summary_embedding','informative_writer_better'])\n",
    "labels = final_data['overall_writer_better']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(X,labels,test_size=0.2,train_size=0.8)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25,train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lr_dist_writer', 'lr_dist_llm', 'bleu_1_writer', 'bleu_2_writer',\n",
       "       'bleu_3_writer', 'bleu_4_writer', 'bleu_1_llm', 'bleu_2_llm',\n",
       "       'bleu_3_llm', 'bleu_4_llm', 'rogue_1_writer', 'rogue_2_writer',\n",
       "       'rogue_L_writer', 'rogue_1_llm', 'rogue_2_llm', 'rogue_L_llm',\n",
       "       'meteor_writer', 'meteor_llm', 'WMD_writer', 'WMD_llm', 'TER_writer',\n",
       "       'TER_llm', 'cosine_writer', 'cosine_llm', 'Writer_Perplexity',\n",
       "       'LLM_Perplexity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "594    1\n",
       "595    0\n",
       "596    2\n",
       "597    2\n",
       "598    2\n",
       "Name: overall_writer_better, Length: 599, dtype: int32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flags' object has no attribute 'c_contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski\u001b[39m\u001b[38;5;124m'\u001b[39m, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[1;32m----> 5\u001b[0m y_pred_knn \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m      6\u001b[0m cm_knn \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred_knn)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (cm_knn)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:246\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    244\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fit_method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ArgKminClassMode\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    247\u001b[0m         X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[0;32m    248\u001b[0m     ):\n\u001b[0;32m    249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:471\u001b[0m, in \u001b[0;36mArgKminClassMode.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_usable_for\u001b[39m(\u001b[38;5;28mcls\u001b[39m, X, Y, metric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True if the dispatcher can be used for the given parameters.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    True if the PairwiseDistancesReduction can be used, else False.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 471\u001b[0m         ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(X, Y, metric)\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;66;03m# TODO: Support CSR matrices.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X)\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(Y)\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;66;03m# TODO: implement Euclidean specialization with GEMM.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    477\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:115\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_sparse_matrix\u001b[39m(X):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    103\u001b[0m         isspmatrix_csr(X)\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    113\u001b[0m is_usable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    114\u001b[0m     get_config()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_cython_pairwise_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(X) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(X))\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(Y) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(Y))\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_metrics()\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_usable\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:99\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for.<locals>.is_numpy_c_ordered\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_numpy_c_ordered\u001b[39m(X):\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Flags' object has no attribute 'c_contiguous'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5,metric = 'minkowski', p = 2)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(x_test)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "print (cm_knn)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print (acc_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  4 15]\n",
      " [ 0 15 42]\n",
      " [ 0  9 35]]\n",
      "0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_rbf = SVC(kernel = 'linear', random_state = 0)\n",
    "svc_rbf.fit(x_train, y_train)\n",
    "y_pred_svc_rbf = svc_rbf.predict(x_test)\n",
    "\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc_rbf)\n",
    "print (cm_svc)\n",
    "acc_svc_rbf = accuracy_score(y_test, y_pred_svc_rbf)\n",
    "print (acc_svc_rbf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  0 16]\n",
      " [ 2  9 46]\n",
      " [ 1  3 40]]\n",
      "0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train, y_train)\n",
    "y_pred = nb.predict(x_test)\n",
    "\n",
    "cm_nb = confusion_matrix(y_test, y_pred)\n",
    "print(cm_nb)\n",
    "acc_nb = accuracy_score(y_test, y_pred)\n",
    "print(acc_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  8  6]\n",
      " [20 20 17]\n",
      " [16 15 13]]\n",
      "0.31666666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "cm_dt = confusion_matrix(y_test, y_pred)\n",
    "print (cm_dt)\n",
    "acc_dt = accuracy_score(y_test, y_pred)\n",
    "print (acc_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3 14]\n",
      " [ 1 18 38]\n",
      " [ 1  7 36]]\n",
      "Accuracy: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42,class_weight=None)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "accuracy = rf.score(x_test, y_test)\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred)\n",
    "print (cm_rf)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  4 11]\n",
      " [ 9 22 26]\n",
      " [10 13 21]]\n",
      "0.39166666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = AdaBoostClassifier()\n",
    "abc.fit(x_train, y_train)\n",
    "\n",
    "y_pred_abc = abc.predict(x_test)\n",
    "cm_ada = confusion_matrix(y_test, y_pred_abc)\n",
    "print (cm_ada)\n",
    "acc_abc = accuracy_score(y_test, y_pred_abc)\n",
    "print (acc_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  1 14]\n",
      " [ 2 11 44]\n",
      " [ 3  3 38]]\n",
      "0.44166666666666665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(x_train, y_train)\n",
    "\n",
    "y_pred_qda = qda.predict(x_test)\n",
    "cm_qda = confusion_matrix(y_test, y_pred_qda)\n",
    "print (cm_qda)\n",
    "acc_qda = accuracy_score(y_test, y_pred_qda)\n",
    "print (acc_qda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 12  6]\n",
      " [ 1 49  7]\n",
      " [ 1 33 10]]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
    "mlp.fit(x_train, y_train)\n",
    "\n",
    "y_pred_mlp = mlp.predict(x_test)\n",
    "cm = confusion_matrix(y_test, y_pred_mlp)\n",
    "print (cm)\n",
    "acc_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print (acc_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------+--------------------+\n",
      "| Classifier                      |   Accuracy | Confusion Matrix   |\n",
      "+=================================+============+====================+\n",
      "| Random forest                   |   0.441667 | [[ 2  3 14]        |\n",
      "|                                 |            |  [ 1 18 38]        |\n",
      "|                                 |            |  [ 1  7 36]]       |\n",
      "+---------------------------------+------------+--------------------+\n",
      "| Linear SVM                      |   0.416667 | [[ 0  4 15]        |\n",
      "|                                 |            |  [ 0 15 42]        |\n",
      "|                                 |            |  [ 0  9 35]]       |\n",
      "+---------------------------------+------------+--------------------+\n",
      "| Adaboost                        |   0.391667 | [[ 4  4 11]        |\n",
      "|                                 |            |  [ 9 22 26]        |\n",
      "|                                 |            |  [10 13 21]]       |\n",
      "+---------------------------------+------------+--------------------+\n",
      "| Quadratic Discriminant Analysis |   0.441667 | [[ 4  1 14]        |\n",
      "|                                 |            |  [ 2 11 44]        |\n",
      "|                                 |            |  [ 3  3 38]]       |\n",
      "+---------------------------------+------------+--------------------+\n",
      "| Decision tree                   |   0.316667 | [[ 5  8  6]        |\n",
      "|                                 |            |  [20 20 17]        |\n",
      "|                                 |            |  [16 15 13]]       |\n",
      "+---------------------------------+------------+--------------------+\n",
      "| GaussianNB                      |   0.433333 | [[ 3  0 16]        |\n",
      "|                                 |            |  [ 2  9 46]        |\n",
      "|                                 |            |  [ 1  3 40]]       |\n",
      "+---------------------------------+------------+--------------------+\n",
      "| MLP                             |   0.5      | [[ 1 12  6]        |\n",
      "|                                 |            |  [ 1 49  7]        |\n",
      "|                                 |            |  [ 1 33 10]]       |\n",
      "+---------------------------------+------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    " \n",
    "mydata = [\n",
    "    [\"Random forest\",acc_rf,cm_rf], \n",
    "    [\"Linear SVM\", acc_svc_rbf,cm_svc], \n",
    "    [\"Adaboost\", acc_abc,cm_ada], \n",
    "    [\"Quadratic Discriminant Analysis\", acc_qda,cm_qda],\n",
    "    [\"Decision tree\",acc_dt,cm_dt],\n",
    "    [\"GaussianNB\",acc_nb,cm_nb],\n",
    "    [\"MLP\",acc_mlp,cm]\n",
    "]\n",
    " \n",
    "head = [\"Classifier\", \"Accuracy\", \"Confusion Matrix\"]\n",
    " \n",
    "# display table\n",
    "print(tabulate(mydata, headers=head, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
