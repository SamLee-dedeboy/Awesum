\section{Introduction}
Text summarization is an extensively studied NLP task that has many downstream applications.
The recent success of Large Language Models (LLMs) opens up even more application scenarios.
With LLMs, a summarization can be easily generated with zero-shot or few-shot prompting.
Prompt-based summarization provides opportunities for injecting user intent into the summarization process and is therefore highly customizable.
Moreover, prompt-based summarization needs zero or few examples, further allowing users to craft examples tailored to their application contexts.
Each prompt template can thus be seen as a summarization system that demonstrates better capability over previous neural models that do not respond to user intents and require a large amount of labeled data to train. 
The development of such prompt-based summarization systems has become a highly iterative process.

However, the evaluation of prompt-based summarization systems remains a challenge.
The most common approach for evaluating summarization systems is to compare the average score of a computational metric such as ROGUE~\cite{lin2004rouge} on a labeled dataset, but such an approach has been criticized in many ways~\cite{deutsch2022re, bhandari2020re, peyrard2017learning, novikova2017we, durmus2020feqa} even in previous neural-based summarization systems.
Evaluating LLM summarization systems using computational metrics is even more questionable.
First, computational metrics do not reliably quantify system improvements among state-of-the-art systems because they do not distinguish high-quality summaries very well~\cite{deutsch2022re, bhandari2020re, novikova2017we}.
Much of the improvements observed over such metrics between a newly proposed system and the baseline system are attributed to successfully distinguishing the `easy' cases, i.e.cases where the baseline system fails very badly.
LLMs have been demonstrated to be able to produce human-level summaries that the quality differences are too nuanced for computational metrics to capture~\cite{zhou2022large}.
Second, computational metrics rely on reference texts (ground-truths) to compute, which are often crafted by human writers.
Such reference texts are expensive to craft and do not generalize well across domains. 
The usage of such reference texts also limits the customizability of LLMs.
More recently, reference-free metrics such as $Q^2$~\cite{honovich2021q2} or FEQA~\cite{durmus2020feqa} have been proposed for evaluation without reference texts.
% The rationale behind these approaches is that a question should have similar answers from the input text and the summarized text.
However, they are reported to be capturing spurious correlations~\cite{durmus2022spurious}.
Another trend of reference-free evaluation is using LLM as evaluators, but they do not yet exhibit a consistent correlation with human judgment~\cite{zhou2022large}.
In addition, all these metrics have a high computation time that is not suitable for an iterative development process.

One commonality among the above approaches is that they are all automatic approaches that completely exclude humans in the evaluation process.
We argue that the capability of LLMs in summarization has reached, if not exceeded, the human level.
Prompt-based summarization can be applied to almost any application scenario, with a high level of customizability.
The quality of a summarization system in real-world scenarios is arguably too complex to be quantified by computational metrics.
Consequently, the evaluation of such summarization systems will likely be incomprehensive without a human in the loop.

In addition, embedding analysis has been shown to be effective in explaining LLM behaviors such as document relevancy ranking~\cite{lucchese2023can, mishra2023promptaid}.
Following this direction, we propose to evaluate summarization systems by analyzing the embedding distribution of the input text and summarized text.
We present experiments to show that the summarization system is essentially a transformation function that maps the input text embeddings to summarized text embeddings.
Thus, the quality of a summarization system can be evaluated by analyzing the embedding distribution of the input text and summarized text.

Following the above discussion, we propose a visual analytic system that incorporates a human-in-the-loop approach to evaluate a summarization system.
Our system provides a visualization of the embedding distribution of the input text and summarized text and interactions to analyze the distributions.
In addition to the distribution visualization, we further incorporate existing computational metrics in the system as complementary signals.
Our contributions are as follows:
\begin{itemize}
   \item We show that embedding analysis is a promising approach for evaluating summarization systems.
   \item We propose to evaluate summarization systems in a human-in-the-loop approach and develop a visual analytic system to support embedding analysis for summarization system evaluation.
   \item We evaluate our system with quantitative experiments and qualitative expert reviews.
\end{itemize}

