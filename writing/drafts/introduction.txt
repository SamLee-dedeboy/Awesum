Introduction
Text summarization is an extensively studied NLP task that has many downstream applications.
The recent success of Large Language Models (LLMs) opens up even more application scenarios, many of which inevitably involve summarization somewhere in the pipeline.
With LLMs, a summarization can be easily generated with zero-shot or few-shot prompting.
Each prompt template can thus be seen as a summarization system that is highly customizable, enabling application developers to develop their summarization systems tailored to the application scenario.
However, the evaluation of summarization systems remains a challenge.
The most common existing approach for evaluating such systems is to compare the average BLEU or ROGUE score on a labeled dataset, but such an approach is criticized in many ways [1, 2, 3, 6, 10] even in previous summarization systems.
Using computational metrics for the evaluation of LLM summarization is even more questionable.
First, computational metrics do to reliably quantify system improvements among state-of-the-art systems because they do not distinguish high-quality summaries very well [1, 2, 6]. 
Much of the improvements observed over such metrics between a newly proposed system and the baseline system are attributed to successfully distinguishing the `easy' cases, i.e.cases where the baseline system fails very badly.
LLMs have been demonstrated to be able to produce human-level summaries that the quality differences are too nuanced for automatic metrics to capture [5].
Second, computational metrics rely on reference texts (ground-truths) to compute, which are often crafted by human writers.
Such reference texts do not exist in zero-shot or few-shot prompting, and the usage of such reference texts can limit the model's performance.
More recently, reference-free metrics such as Q^2 or FEQA have been proposed for evaluation without reference texts.
The rationale behind these approaches is that a question should have similar answers from the input text and the summarized text.
However, they are reported to be capturing spurious correlations [8].
Another trend of evaluation is using LLM as evaluators, but they do not exhibit a consistent correlation with human judgment [5].

One commonality among the above approaches is that they are all automatic approaches that completely exclude humans in the evaluation process.
We argue that the capability of LLMs in summarization has reached, if not exceeded, the human level.
LLMs summarization can be applied to almost any application scenario, with a high level of customizability.
The evaluation of such summarization systems will likely be incomprehensive without a human in the loop.
Thus, we propose a visual analytic system that incorporates a human-in-the-loop approach to evaluate an LLM summarization system.
The system visualizes the latent space distributions of the input text and summarized text. 
The rationale behind our approach is that input text and summarized text should have similar relative distances. 
Outliers observed in the summarized text distributions are likely badly summarized texts.
In addition to the distribution visualization, we further incorporate existing computational metrics in the system as complementary signals.
We evaluate our system with expert reviews.





Outline
1. Existing problems with using automatic summarization metrics to guide prompt optimization/evaluate prompts:
    - LLM produces human-level summaries that quality differences are too nuanced for automatic metrics to capture [5]
    - Correlation with human judgment is questionable [6]
    - A trade-off between abstractive and faithfulness must be made for automatic metrics [10]
    - automatic metrics tend to correlate well with humans at the system level but have poor correlations at the instance level [2, 3]
    - No single metric can outperform (correlation) across datasets -> This suggests different datasets need to use different metrics [2]
    - Metrics can not reliably quantify improvements if the difference is too small -- much success is attributed to ranking `easy' cases [1, 2, 6]

2. Problems with LLM evaluators: outperforms automatic metrics, but do not exhibit consistent correlation with human judgment [5]
3. Problems with reference-free metrics: capturing spurious correlation (highly correlated with spurious metrics like text length, word overlap, and perplexity) [8]
4. LLM evaluators and reference-free metrics have higher computation time, 

Notes:
    1. LLM benchmarks are neither informative nor actionable for application developers
    2. evaluation of a new metric: system-level vs. instance level: which one would visual metric succeed at?
    3. reference-based metrics are not able to capture factuality (faithfulness) errors [7]
    4. Meaningful -> Grammatical/Readable/Formal -> Faithful -> Capturing gist

[1] Deutsch, D., Dror, R., & Roth, D. (2022). Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics (arXiv:2204.10216). arXiv. http://arxiv.org/abs/2204.10216
[2] Bhandari, M., Gour, P. N., Ashfaq, A., Liu, P., & Neubig, G. (2020). Re-evaluating Evaluation in Text Summarization. In B. Webber, T. Cohn, Y. He, & Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 9347–9359). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.emnlp-main.751
[3] Peyrard, M., Botschen, T., & Gurevych, I. (2017). Learning to Score System Summaries for Better Content Selection Evaluation. In L. Wang, J. C. K. Cheung, G. Carenini, & F. Liu (Eds.), Proceedings of the Workshop on New Frontiers in Summarization (pp. 74–84). Association for Computational Linguistics. https://doi.org/10.18653/v1/W17-4510
[4] Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., & Hashimoto, T. B. (2023). Benchmarking Large Language Models for News Summarization (arXiv:2301.13848). arXiv. http://arxiv.org/abs/2301.13848
[5] Shen, C., Cheng, L., Nguyen, X.-P., You, Y., & Bing, L. (2023). Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization (arXiv:2305.13091). arXiv. http://arxiv.org/abs/2305.13091
[6] Novikova, J., Dušek, O., Cercas Curry, A., & Rieser, V. (2017). Why We Need New Evaluation Metrics for NLG. In M. Palmer, R. Hwa, & S. Riedel (Eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2241–2252). Association for Computational Linguistics. https://doi.org/10.18653/v1/D17-1238
[7] Falke, T., Ribeiro, L. F. R., Utama, P. A., Dagan, I., & Gurevych, I. (2019). Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. In A. Korhonen, D. Traum, & L. Màrquez (Eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 2214–2220). Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1213
[8] Durmus, E., Ladhak, F., & Hashimoto, T. (2022). Spurious Correlations in Reference-Free Evaluation of Text Generation. In S. Muresan, P. Nakov, & A. Villavicencio (Eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1443–1454). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.102
[9] Wang, A., Cho, K., & Lewis, M. (2020). Asking and Answering Questions to Evaluate the Factual Consistency of Summaries. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 5008–5020). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.450
[10] Durmus, E., He, H., & Diab, M. (2020). FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 5055–5070). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.454



